[["index.html", "Time Series Analysis and Its Applications Chapter 1 Introduction", " Time Series Analysis and Its Applications Steven Golovkine 2021-01-31 Chapter 1 Introduction This book aims to provide my results to the different problems within Time Series Analysis and Its Application, with R examples, by Robert H. Shumway and David S. Stoffer (Shumway and Stoffer 2011). The applied exercises will be solved using the language Julia (https://julialang.org) when it is possible. This book is compiled using R Markdown and bookdown (https://github.com/rstudio/bookdown). "],["characteristics-of-time-series.html", "Chapter 2 Characteristics of Time Series 2.1 Exercices", " Chapter 2 Characteristics of Time Series 2.1 Exercices # Load packages using Distributions using PyPlot using RData using StatsBase matplotlib.rcParams[&quot;text.usetex&quot;] = true; 2.1.1 Exercices 1.1 # Load data EQ5 = load(&quot;../data/EQ5.rda&quot;) EXP6 = load(&quot;../data/EXP6.rda&quot;); # Plot the data figure(figsize=(15, 5)) plot(EXP6[&quot;EXP6&quot;], label=&quot;Explosion&quot;) plot(EQ5[&quot;EQ5&quot;], label=&quot;Earthquake&quot;) xlabel(&quot;Time&quot;) legend() title(&quot;Arrival phase from an earthquake and explosion&quot;) show() We observe that the earthquake follows the explosion. There should be a high correlation between the moment of the explosion and few moments later for the earthquake. Secondly, the intensity of the earthquake might depend on the intensity of the explosion. 2.1.2 Exercices 1.2 Let’s assume the following signal-plus-noise model: \\[x_t = s_t + w_t, \\quad w_t \\sim \\mathcal{N}(0, 1)\\] Question (a) \\(x_t = s_t + w_t\\), for \\(t = 1, \\dots, 200\\), where \\[s_t = \\left\\{ \\begin{array}{l l} 0, &amp; t = 1, \\dots, 100 \\\\ 10\\exp\\left(-\\frac{(t - 100)}{20}\\right)\\cos(2\\pi t / 4), &amp; t = 101, \\dots, 200 \\end{array} \\right.\\] function signal_a(l) return [t &lt; 101 ? 0 : 10 * exp(-(t - 100) / 20) * cos(2 * pi * t / 4) for t in l] end function signal_plus_noise_a(l) w = rand(Normal(), length(l)) return signal_a(l) + w end signal_plus_noise_a (generic function with 1 method) figure(figsize=(15, 5)) plot(1:0.2:200, signal_plus_noise_a(1:0.2:200)) xlabel(L&quot;$t$&quot;) ylabel(L&quot;$x_t$&quot;) title(L&quot;x_t = s_t + w_t&quot;) show() Question (b) \\(x_t = s_t + w_t\\), for \\(t = 1, \\dots, 200\\), where \\[s_t = \\left\\{ \\begin{array}{l l} 0, &amp; t = 1, \\dots, 100 \\\\ 10\\exp\\left(-\\frac{(t - 100)}{200}\\right)\\cos(2\\pi t / 4), &amp; t = 101, \\dots, 200 \\end{array} \\right.\\] function signal_b(l) return [t &lt; 101 ? 0 : 10 * exp(-(t - 100) / 200) * cos(2 * pi * t / 4) for t in l] end function signal_plus_noise_b(l) w = rand(Normal(), length(l)) return signal_b(l) + w end signal_plus_noise_b (generic function with 1 method) figure(figsize=(15, 5)) plot(1:0.2:200, signal_plus_noise_b(1:0.2:200)) xlabel(L&quot;$t$&quot;) ylabel(L&quot;$x_t$&quot;) title(L&quot;x_t = s_t + w_t&quot;) show() Question (c) The serie (a) appears to be quite close to the explosion serie and the serie (b) appears to be quite close to the earthquake serie. Let’s plot the signal modulators (a) \\(\\exp(-t / 20)\\) and (b) \\(\\exp(-t / 200)\\), for \\(t = 1, \\dots, 100\\). figure(figsize=(15, 5)) plot([exp(-t / 20) for t in 1:100], label=L&quot;\\exp(-t / 20)&quot;) plot([exp(-t / 200) for t in 1:100], label=L&quot;\\exp(-t / 200)&quot;) xlabel(L&quot;$t$&quot;) title(&quot;Signal modulators&quot;) legend() show() The decreasing of the intensity of the first signal modulators is way larger than the second one. 2.1.3 Exercices 1.3 Question (a) Consider the autoregression model: \\[x_t = -0.9x_{t-2} + w_t, \\quad w_t \\sim \\mathcal{N}(0, 1).\\] First, we will define a recursive filter function and a convolution filter function (based on the C function of the R package stats). function filter_recursive(x, filter) nx = length(x) nf = length(filter) out = vcat(x, Array{Float64}(undef, nf)) for i in 1:nx Σ = x[i] for j in 1:nf tmp = out[i + nf - j] Σ += filter[j] * tmp end out[i + nf] = Σ end return out[(nf + 1):end] end function filter_convolution(x, filter, sides) nx = length(x) nf = length(filter) nshift = (sides == 1) ? 0 : div(nf, 2) out = Array{Float64}(undef, nx) for i in 1:nx z = 0 if (i + nshift - nf &lt; 0) || (i + nshift &gt; nx) out[i] = NaN continue end for j in (max(1, i + nshift - nx)):(min(nf, i + nshift - 1)) tmp = x[i + nshift - j] z += filter[j] * tmp end out[i] = z end return out end filter_convolution (generic function with 1 method) w = rand(Normal(), 500) x = filter_recursive(w, [0, -0.9]) x_filter = filter_convolution(x, [0.25, 0.25, 0.25, 0.25], 1); # Plot the time series figure(figsize=(15, 5)) plot(x, label=L&quot;x_t&quot;) plot(x_filter, label=L&quot;v_t&quot;) xlabel(L&quot;$t$&quot;) title(L&quot;x_t = -0.9x_{t-2} + w_t&quot;) legend() show() The behavior of \\(x_t\\) is very erratic, even if we might see some patterns within. The moving average filter completely change that behavior and smooth the serie such that no patterns are discernable. Question (b) Consider the model: \\[x_t = \\cos(2\\pi t / 4).\\] x = [cos(2 * pi * t / 4) for t in 1:0.2:100] x_filter = filter_convolution(x, [0.25, 0.25, 0.25, 0.25], 1); # Plot the time series figure(figsize=(15, 5)) plot(1:0.2:100, x, label=L&quot;x_t&quot;) plot(1:0.2:100, x_filter, label=L&quot;v_t&quot;) xlabel(L&quot;$t$&quot;) title(L&quot;x_t = \\cos(2\\pi t / 4)&quot;) legend() show() The moving average filter has the effect of shifting the signal in this case. Question (c) Consider the model: \\[x_t = \\cos(2\\pi t / 4) + w_t, \\quad w_t \\sim \\mathcal{N}(0, 1).\\] w = rand(Normal(), 496) x = [cos(2 * pi * t / 4) for t in 1:0.2:100] + w x_filter = filter_convolution(x, [0.25, 0.25, 0.25, 0.25], 1); # Plot the time series figure(figsize=(15, 5)) plot(1:0.2:100, x, label=L&quot;x_t&quot;) plot(1:0.2:100, x_filter, label=L&quot;v_t&quot;) xlabel(L&quot;$t$&quot;) title(L&quot;x_t = \\cos(2\\pi t / 4) + w_t&quot;) legend() show() Here, the moving average filter removes a bit the noise in the signal to recover the underlying “true” form of the signal. Question (d) The filtering behaves differently for the different examples. For the first one, as the signal is basically just noise, the filtering will remove this component and so, return something close to 0. For the second one, there is no noise is the signal, the filtering insert a time lag in the signal. And for the last one, the filtering performs both the denoising and the shift of the signal. To obtain just a denoising, the filtering should be centered around the value. Something like, \\[v_t = (x_{t+2} + x_{t+1} + x_{t-1} + x_{t-2}) / 4.\\] 2.1.4 Exercices 1.4 Show that the autocovariance function can be written as \\[\\gamma(s, t) = \\mathbb{E}(x_sx_t) - \\mu_s\\mu_t, \\quad\\text{where}\\quad \\mathbb{E}(x_t) = \\mu_t.\\] \\[\\begin{align} \\gamma(s, t) &amp;= \\mathbb{E}\\left((x_s - \\mu_s)(x_t - \\mu_t\\right) \\\\ &amp;= \\mathbb{E}\\left(x_sx_t - \\mu_sx_t - x_s\\mu_t + \\mu_s\\mu_t\\right) \\\\ &amp;= \\mathbb{E}(x_sx_t) - \\mu_s\\mathbb{E}(x_t) - \\mu_t\\mathbb{E}(x_s) + \\mu_s\\mu_t \\\\ &amp;= \\mathbb{E}(x_sx_t) - \\mu_s\\mu_t \\end{align}\\] 2.1.5 Exercices 1.5 Recall the two series in the exercise 1.2: First serie: \\(x_t = s_t + w_t\\), for \\(t = 1, \\dots, 200\\), where \\[s_t = \\left\\{ \\begin{array}{l l} 0, &amp; t = 1, \\dots, 100 \\\\ 10\\exp\\left(-\\frac{(t - 100)}{20}\\right)\\cos(2\\pi t / 4), &amp; t = 101, \\dots, 200 \\end{array} \\right.\\] Second serie: \\(x_t = s_t + w_t\\), for \\(t = 1, \\dots, 200\\), where \\[s_t = \\left\\{ \\begin{array}{l l} 0, &amp; t = 1, \\dots, 100 \\\\ 10\\exp\\left(-\\frac{(t - 100)}{200}\\right)\\cos(2\\pi t / 4), &amp; t = 101, \\dots, 200 \\end{array} \\right.\\] Question (a) We are in the case of signal plus noise model. So, in both serie, the mean functions will be \\(s(t)\\). # Plot the mean functions figure(figsize=(15, 5)) plot(1:0.2:200, signal_a(1:0.2:200), label=L&quot;First~ s_t&quot;) plot(1:0.2:200, signal_b(1:0.2:200), label=L&quot;Second~ s_t&quot;) xlabel(L&quot;$t$&quot;) title(&quot;Mean functions&quot;) legend() show() Question (b) The autocovariance of the series will be the same, that is: \\[\\begin{align} \\gamma(s,t) &amp;= Cov(x_s, x_t) \\\\ &amp;= Cov(s_s + w_s, s_t + w_t) \\\\ &amp;= Cov(s_s, s_t) + Cov(s_s, w_t) + Cov(s_t, w_s) + Cov(w_s, w_t) \\\\ &amp;= Cov(w_s, w_t) \\\\ &amp;= \\left\\{ \\begin{array}{r c l} 1 &amp;if&amp; s = t\\\\ 0 &amp;if&amp; s \\neq t \\\\ \\end{array} \\right. \\end{align}\\] 2.1.6 Exercices 1.6 Consider the time series \\[x_t = \\beta_1 + \\beta_2t + w_t,\\] where \\(\\beta_1\\) and \\(\\beta_2\\) are known constants and \\(w_t\\) is a white noise process with variance \\(\\sigma^2_w\\). Question (a) The serie \\(x_t\\) is not (weakly) stationary because its mean function depends on time \\(t\\): \\[\\mu_x(t) = \\beta_1 + \\beta_2t.\\] Question (b) Define the process \\(y_t = x_t - x_{t - 1} = \\beta_2 + w_t - w_{t - 1}\\). The mean function of the process \\(y_t\\) is \\(\\beta_2\\) and, so, does not depend on time \\(t\\). The autocovariance is: \\[\\begin{align} \\gamma(s, t) &amp;= Cov(y_s, y_t) \\\\ &amp;= Cov(\\beta_2 + w_s - w_{s-1}, \\beta_2 + w_t - w_{t-1}) \\\\ &amp;= Cov(w_s, w_t) - Cov(w_s, w_{t-1}) - Cov(w_{s-1}, w_t) + Cov(w_{s-1}, w_{t-1}) \\\\ &amp;= \\left\\{ \\begin{array}{r c l} 2\\sigma_w^2 &amp;if&amp; s - t = 0\\\\ -\\sigma_w^2 &amp;if&amp; \\vert s - t \\vert = 1\\\\ \\end{array} \\right. \\end{align}\\] So, the autocovariance only depends on the lag between \\(s\\) and \\(t\\) and not the absolute location of the points along the serie. Finally, as the variance is finite, the serie is (weakly) stationary. Question (c) The mean of the moving average is \\[\\begin{align} \\mathbb{E}(v_t) &amp;= \\mathbb{E}\\left(\\frac{1}{2q + 1}\\sum_{j=-q}^{q}x_{t-j}\\right) \\\\ &amp;= \\frac{1}{2q + 1}\\sum_{j=-q}^{q}\\mathbb{E}(x_{t-j}) \\\\ &amp;= \\frac{1}{2q + 1}\\sum_{j=-q}^{q}\\mathbb{E}(\\beta_1 + \\beta_2(t-j) + w_{t-j}) \\\\ &amp;= \\beta_1 + \\beta_2t - \\frac{\\beta_2}{2q+1}\\sum_{j=-q}^{q}j \\\\ &amp;= \\beta_1 + \\beta_2t \\end{align}\\] The autocovariance function is, for all \\(h\\), \\[\\begin{align} \\gamma(h) &amp;= Cov(v_t, v_{t + h}) \\\\ &amp;= Cov\\left(\\frac{1}{2q + 1}\\sum_{j=-q}^{q}x_{t-j}, \\frac{1}{2q + 1}\\sum_{i=-q}^{q}x_{t-i+h}\\right) \\\\ &amp;= \\left(\\frac{1}{2q + 1}\\right)^2 Cov\\left(\\sum_{j=-q}^{q}w_{t-j},\\sum_{i=-q}^{q}w_{t-i+h}\\right) \\\\ &amp;= \\left(\\frac{\\sigma}{2q + 1}\\right)^2 \\sum_{i, j = -q}^q \\mathbb{1}(i - j = h) \\\\ &amp;= \\left\\{ \\begin{array}{r c l} \\left(\\frac{\\sigma}{2q + 1}\\right)^2 &amp;if&amp; \\lvert h \\rvert \\leq 2q\\\\ 0 &amp;if&amp; \\lvert h \\rvert &gt; 2q\\\\ \\end{array} \\right. \\end{align}\\] 2.1.7 Exercices 1.7 Consider the moving average process \\[x_t = w_{t-1} + 2w_t + w_{t+1},\\] where \\(w_t\\) are independent with zero means and variacne \\(\\sigma_w^2\\). The autocovariance function is \\[\\begin{align} \\gamma(h) &amp;= Cov(x_t, x_{t+h}) \\\\ &amp;= Cov(w_{t-1} + 2w_t - w_{t+1}, w_{t-1+h} + 2w_{t+h} - w_{t+1+h}) \\\\ &amp;= \\left\\{ \\begin{array}{r c l} 6\\sigma_w^2 &amp;if&amp; h = 0\\\\ 4\\sigma_w^2 &amp;if&amp; \\vert h \\vert = 1\\\\ \\sigma_w^2 &amp;if&amp; \\vert h \\vert = 2\\\\ 0 &amp;if&amp; \\vert h \\vert &gt; 2\\\\ \\end{array} \\right. \\end{align}\\] The autocorrelation function is \\[\\begin{align} \\rho(h) &amp;= \\gamma(h) / \\gamma(0) \\\\ &amp;= \\left\\{ \\begin{array}{r c l} 1 &amp;if&amp; h = 0\\\\ 2/3 &amp;if&amp; \\vert h \\vert = 1\\\\ 1/6 &amp;if&amp; \\vert h \\vert = 2\\\\ 0 &amp;if&amp; \\vert h \\vert &gt; 2\\\\ \\end{array} \\right. \\end{align}\\] # Plot the ACF figure(figsize=(15, 5)) stem(-4:4, [0, 0, 1/6, 2/3, 1, 2/3, 1/6, 0, 0], use_line_collection=true) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() 2.1.8 Exercices 1.8 Consider the random walk with drift model \\[x_t = \\delta + x_{t-1} + w_t, \\quad t = 1, 2, \\dots\\] with \\(x_0 = 0\\), where \\(w_t\\) is white noise with variance \\(\\sigma_w^2\\). Question (a) Proof by induction: - \\(x_1 = \\delta + x_0 + w_1 = \\delta + w_1\\) - Assume that the relation is true for \\(x_t\\), we have \\[x_{t + 1} = \\delta + x_t + w_{t+1} = \\delta + \\left(\\delta t + \\sum_{k=1}^t w_k\\right) + w_{k+1} = \\delta(t+1) + \\sum_{k=1}^{t+1} w_k.\\] Question (b) The mean function of \\(x_t\\) is \\[\\mathbb{E}(x_t) = \\mathbb{E}\\left(\\delta t + \\sum_{k=1}^{t}w_{k}\\right) = \\delta t + \\sum_{k=1}^{t}\\mathbb{E}(w_{k}) = \\delta t\\] The autocovariance function of \\(x_t\\) is \\[\\gamma(s, t) = Cov(x_s, x_t) = Cov\\left(\\sum_{k=1}^s w_k, \\sum_{j=1}^t w_j\\right) = \\sum_{k=1}^s \\sum_{j=1}^t Cov(w_k, w_j) = \\min(s, t)\\sigma_w^2.\\] Question (c) \\(x_t\\) is not stationary because its mean function depends on \\(t\\) and its autocovariance function depends on the particular time values \\(s\\) and \\(t\\), and not on the time separation \\(\\lvert s - t \\rvert\\). Question (d) \\[\\rho(t-1, t) = \\frac{\\gamma(t-1, t)}{\\sqrt{\\gamma(t-1, t-1)\\gamma(t, t)}} = \\frac{(t-1)\\sigma^2}{\\sqrt{(t-1)\\sigma^2(t)\\sigma^2}} = \\sqrt{\\frac{t-1}{t}} = \\sqrt{1 - \\frac{1}{t}} \\xrightarrow[t \\rightarrow \\infty]{} 1\\] It means that, when \\(t\\) goes to infinity, \\(x_t\\) and \\(x_{t-1}\\) are almost exactly the same thing. Intuitively, if we plot \\(x_t\\) against \\(x_{t-1}\\), we should obtain a straight line that pass through \\((0, 0)\\). Question (e) Consider the serie: \\(y_t = x_t - x_{t-1} = \\delta + w_t\\). Then, the serie is stationary because: \\(\\mathbb{E}(y_t) = \\delta\\) and \\(\\gamma(s, t) = \\sigma^2\\mathbb{1}(s = t)\\). 2.1.9 Exercices 1.9 Consider the serie \\[x_t = U_1\\sin(2\\pi\\omega_0t) + U_2\\cos(2\\pi\\omega_0t)\\] where \\(U_1\\) and \\(U_2\\) are independent random variables with zero means and \\(\\mathbb{E}(U_1^2) = \\mathbb{E}(U_2^2) = \\sigma^2\\). The mean function is \\[\\mathbb{E}(x_t) = \\mathbb{E}(U_1\\sin(2\\pi\\omega_0t) + U_2\\cos(2\\pi\\omega_0t)) = \\sin(2\\pi\\omega_0t)\\mathbb{E}(U_1) + \\cos(2\\pi\\omega_0t)\\mathbb{E}(U_2) = 0.\\] The autocovariance function is \\[\\begin{align} \\gamma(h) &amp;= Cov(x_t, x_{t+h}) \\\\ &amp;= Cov(U_1\\sin(2\\pi\\omega_0t) + U_2\\cos(2\\pi\\omega_0t), U_1\\sin(2\\pi\\omega_0(t+h)) + U_2\\cos(2\\pi\\omega_0(t+h)))\\\\ &amp;= \\sin(2\\pi\\omega_0t)\\sin(2\\pi\\omega_0(t+h))Cov(U_1, U_1) + \\cos(2\\pi\\omega_0t)\\cos(2\\pi\\omega_0(t+h))Cov(U_2, U_2)\\\\ &amp;= \\sigma^2\\cos(2\\pi\\omega_0t - 2\\pi\\omega_0(t+h))\\\\ &amp;= \\sigma^2\\cos(2\\pi\\omega_0h) \\end{align}\\] Thus, the serie is weakly stationary with the given autocovariance function. 2.1.10 Exercices 1.10 Suppose we would like to predict a single stationary series \\(x_t\\) with zero mean and autocorrelation function \\(gamma(h)\\) at some time in the future \\(t + l\\), for \\(l &gt; 0\\). Question (a) Compute the derivative of \\(MSE(A)\\) with respect to \\(A\\): \\[\\frac{\\partial MSE(A)}{\\partial A} = -2\\mathbb{E}(x_tx_{t+l}) + 2A\\mathbb{E}(x_t^2).\\] By setting the derivative to \\(0\\): \\[\\frac{\\partial MSE(A)}{\\partial A} = 0 \\Longleftrightarrow A = \\frac{\\mathbb{E}(x_tx_{t+l})}{\\mathbb{E}(x_t^2)} = \\frac{Cov(x_t, x_{t+l})}{Var(x_t)} = \\frac{\\gamma(l)}{\\gamma(0)} = \\rho(l).\\] Question (b) \\(MSE(\\rho(l)) = \\mathbb{E}(x_{t+l}^2) - 2\\rho(l)\\mathbb{E}(x_tx_{t+l}) + \\rho(l)^2\\mathbb{E}(x_t^2) = \\gamma(0)\\left[1 - \\rho(l)^2\\right]\\) Question (c) Assume \\(x_{t+l} = Ax_t\\). Then, \\[\\begin{align} MSE(\\rho(l)) = 0 &amp;\\Longleftrightarrow&amp; 1 - \\rho(l)^2 = 0 \\\\ &amp;\\Longleftrightarrow&amp; \\lvert \\rho(l) \\rvert = 1 \\\\ &amp;\\Longleftrightarrow&amp; \\left\\{ \\begin{array}{r c l} \\rho(l) = 1 &amp;if&amp; A &gt; 0\\\\ \\rho(l) = -1 &amp;if&amp; A &lt; 1\\\\ \\end{array} \\right. \\end{align}\\] 2.1.11 Exercices 1.11 Consider the process \\[x_t = \\mu + \\sum_{j = -\\infty}^{\\infty} \\phi_j\\omega_{t-j}, \\quad \\sum_{j=\\infty}^{\\infty}\\lvert \\phi_j \\rvert &lt; +\\infty\\] where \\(\\omega_t \\sim \\mathcal{N}(0, \\sigma^2)\\). Question (a) The autocovariance function is, for \\(h \\leq 0\\), \\[\\begin{align} \\gamma(h) &amp;= Cov(x_{t+h}, x_t) \\\\ &amp;= Cov\\left(\\sum_{j = -\\infty}^{\\infty} \\phi_j\\omega_{t-j+h}, \\sum_{k = -\\infty}^{\\infty} \\phi_k\\omega_{t-k}\\right) \\\\ &amp;= \\sum_{j = -\\infty}^{\\infty} \\sum_{k = -\\infty}^{\\infty} \\phi_j\\phi_kCov(\\omega_{t-j+h}, \\omega_{t-k})\\\\ &amp;= \\sigma^2\\sum_{j = -\\infty}^{\\infty} \\phi_{k+h}\\phi_k \\end{align}\\] And by symmetry, we have \\(\\gamma(h) = \\gamma(-h)\\). Question (b) Define \\(x_t^q = \\mu + \\sum_{j = -q}^{q} \\phi_j\\omega_{t-j}\\). In order to prove that \\(x_t\\) exists as a limit in mean square, we would prove that \\(x_t^q \\longrightarrow x_t\\). We have to show that \\(\\mathbb{E}\\lvert x_t^p - x_t^q \\rvert^2 \\longrightarrow 0, p &gt; q &gt; 0\\). \\[\\begin{align} \\mathbb{E}\\lvert x_t^p - x_t^q \\rvert^2 &amp;= \\mathbb{E}\\left\\lvert \\sum_{j = -p}^{p} \\phi_j\\omega_{t-j} - \\sum_{j = -q}^{q} \\phi_j\\omega_{t-j} \\right\\rvert^2 \\\\ &amp;= \\mathbb{E}\\left\\lvert \\sum_{q \\leq \\lvert j \\rvert \\leq p} \\phi_j\\omega_{t-j} \\right\\rvert^2 \\\\ &amp;= \\sum_{q \\leq \\lvert j \\rvert \\leq p}\\sum_{q \\leq \\lvert j \\rvert \\leq p}\\phi_j\\phi_k\\mathbb{E}(\\omega_{t-j}\\omega_{t-k})\\\\ &amp;\\leq \\sum_{q \\leq \\lvert j \\rvert \\leq p}\\sum_{q \\leq \\lvert j \\rvert \\leq p}\\lvert\\phi_j\\rvert\\lvert\\phi_k\\rvert\\lvert\\mathbb{E}(\\omega_{t-j}\\omega_{t-k})\\rvert\\\\ &amp;\\leq \\sum_{q \\leq \\lvert j \\rvert \\leq p}\\sum_{q \\leq \\lvert j \\rvert \\leq p}\\lvert\\phi_j\\rvert\\lvert\\phi_k\\rvert\\mathbb{E}(\\omega_{t-j}^2)^{1/2}\\mathbb{E}(\\omega_{t-k}^2)^{1/2}, \\text{(Cauchy-Schwarz)}\\\\ &amp;\\leq \\sum_{q \\leq \\lvert j \\rvert \\leq p}\\sum_{q \\leq \\lvert j \\rvert \\leq p}\\lvert\\phi_j\\rvert\\lvert\\phi_k\\rvert(\\sigma^2 + \\mu^2)^{1/2}(\\sigma^2 + \\mu^2)^{1/2}\\\\ &amp;= (\\sigma^2 + \\mu^2)\\left(\\sum_{q \\leq \\lvert j \\rvert \\leq p}\\lvert\\phi_j\\rvert\\right)^2 \\longrightarrow 0 \\end{align}\\] Let \\(S\\) denotes the mean square limit of \\(x_t^q\\), then using Fatou’s lemma: \\[\\mathbb{E}(\\lvert S - x_t\\rvert^2) = \\mathbb{E}(\\lim\\inf \\lvert S - x_t^q \\rvert^2) \\leq \\lim\\inf\\mathbb{E}\\lvert S - x_t^q\\rvert^2 = 0.\\] And so, \\(x_t\\) exists as a limit in mean square. 2.1.12 Exercices 1.12 Consider two weakly stationary series \\(x_t\\) and \\(y_t\\). Showing that \\(\\rho_{xy}(h) = \\rho_{yx}(-h)\\) is equivalent to show \\(\\gamma_{xy}(h) = \\gamma_{yx}(-h)\\). Then, \\[\\gamma_{xy}(h) = Cov(x_{t+h}, y_t) = Cov(x_u, y_{u-h}) = Cov(y_{u-h}, x_u) = \\gamma_{yx}(-h).\\] The main part of the proof is done by substitution: \\(u = t + h\\). 2.1.13 Exercices 1.13 Consider the two series: \\[x_t = w_t \\quad\\text{and}\\quad y_t = w_t - \\theta w_{t-1} + u_t,\\] where \\(w_t\\) and \\(u_t\\) are independent white noise series with variance \\(\\sigma_w^2\\) and \\(\\sigma_u^2\\), respectively, and \\(\\theta\\) is an unspecified constant. Question (a) \\[\\begin{align*} \\gamma_y(h) &amp;= Cov(y_{t+h}, y_t) \\\\ &amp;= Cov(w_{t+h} - \\theta w_{t-1+h} + u_{t+h}, w_t - \\theta w_{t-1} + u_t) \\\\ &amp;= \\left\\{ \\begin{array}{c l l} (1 + \\theta^2)\\sigma_w^2 + \\sigma_u^2 &amp;\\text{if}&amp; h = 0\\\\ -\\theta\\sigma_w^2 &amp;\\text{if}&amp; |h| = 1 \\\\ 0 &amp;\\text{else}&amp; \\end{array} \\right. \\end{align*}\\] So, \\[\\rho_y(h) = \\left\\{ \\begin{array}{c l l} 1 &amp;\\text{if}&amp; h = 0\\\\ -\\frac{\\theta\\sigma_w^2}{(1 + \\theta^2)\\sigma_w^2 + \\sigma_u^2} &amp;\\text{if}&amp; |h| = 1 \\\\ 0 &amp;\\text{else}&amp; \\end{array} \\right.\\] Question (b) We have that \\[\\rho_{xy}(h) = \\frac{\\gamma_{xy}(h)}{\\sqrt{\\gamma_x(0)\\gamma_y(0)}}, \\quad \\gamma_x(0) = \\sigma_w^2, \\quad \\gamma_y(0) = (1 + \\theta^2)\\sigma_w^2 + \\sigma_u^2\\] and \\[\\gamma_{xy}(h) = Cov(x_{t+h}, y_t) = Cov(w_{t+h}, w_t - \\theta w_{t-1} + u_t) = \\left\\{ \\begin{array}{c l l} \\sigma_w^2 &amp;\\text{if}&amp; h = 0\\\\ -\\theta\\sigma_w^2 &amp;\\text{if}&amp; h = -1 \\\\ 0 &amp;\\text{else}&amp; \\end{array} \\right.\\] Question (c) \\(x_t\\) and \\(y_t\\) are jointly stationary because \\(\\rho_{xy}(h)\\) is a function only of lag \\(h\\). 2.1.14 Exercices 1.14 Let \\(x_t\\) be a stationary normal process with mean \\(\\mu_x\\) and autocovariance function \\(\\gamma(h)\\). Define the nonlinear time series \\(y_t = \\exp(x_t)\\). Question (a) The serie \\(y_t\\) is defined as the moment generating function of \\(x_t\\) evaluated at \\(1\\), \\(\\mathbb{E}(y_t) = M_x(1)\\). \\[\\mathbb{E}(y_t) = \\mathbb{E}(\\exp(x_t)) = \\exp(\\mu + \\gamma(0)/2).\\] Question (b) The sum of the two normal random variables \\(x_{t+h} + x_t\\) is a normal random variable. Thus, \\[x_{t+h} + x_t \\sim \\mathcal{N}(2\\mu_x, \\gamma(h,h) + \\gamma(0) + 2\\gamma(h)).\\] \\[\\begin{align*} Cov(y_{t+h}, y_t) &amp;= \\mathbb{E}\\left((y_{t+h} - \\mathbb{E}(y_{t+h}))(y_{t} - \\mathbb{E}(y_t))\\right) \\\\ &amp;= \\mathbb{E}(y_{t+h}y_t) - \\mathbb{E}(y_{t+h})\\mathbb{E}(y_t) \\\\ &amp;= \\mathbb{E}(\\exp(x_{t+h} + x_t)) - \\exp(\\mu_x + \\gamma(0)/2)^2 \\\\ &amp;= \\exp(2\\mu_x + (\\gamma(h,h) + \\gamma(0) + 2\\gamma(h)/2) - \\exp(\\mu_x + \\gamma(0)/2)^2 \\\\ &amp;= \\exp(2\\mu_x)\\left(\\exp\\left(\\frac{1}{2}(\\gamma(h,h) + \\gamma(0) + 2\\gamma(h))\\right) - \\exp(\\gamma(0))\\right) \\end{align*}\\] 2.1.15 Exercices 1.15 Let \\(w_t\\) be a normal white process, and consider the series \\[x_t = w_tw_{t-1}.\\] Mean \\[\\mathbb{E}(x_t) = \\mathbb{E}(w_tw_{t-1}) = \\mathbb{E}(w_t)\\mathbb{E}(w_{t-1}) = 0\\] Autocovariance \\[Cov(x_t{t+h}, x_t) = Cov(w_{t+h}w_{t+h-1}, w_tw_{t-1}) = \\mathbb{E}(w_{t+h}w_{t+h-1}w_tw_{t-1}) = \\left\\{ \\begin{array}{c l l} \\sigma_w^4 &amp;\\text{if}&amp; h = 0\\\\ 0 &amp;\\text{else}&amp; \\end{array} \\right.\\] The serie \\(x_t\\) is stationary because the mean function is constant and the autocovariance function only depends on the time lag \\(h\\). 2.1.16 Exercices 1.16 Consider the series \\(x_t = \\sin(2\\pi Ut)\\), where \\(U\\) has a uniform distribution on the interval \\((0,1)\\). Question (a) \\[\\begin{align*} \\mathbb{E}(x_t) &amp;= \\mathbb{E}(\\sin(2\\pi Ut)) \\\\ &amp;= \\int_0^1 \\sin(2\\pi ut)du \\\\ &amp;= \\frac{1}{2\\pi t}\\left(1 - \\cos(2\\pi t)\\right) \\end{align*}\\] As \\(t = 1, 2, \\dots\\), \\(\\cos(2\\pi t) = 1\\), and thus, \\(\\mathbb{E}(x_t) = 0\\). If \\(h \\neq 0\\), then \\[\\begin{align*} \\gamma(h) &amp;= Cov(x_{t+h}, x_t) \\\\ &amp;= Cov(\\sin(2\\pi U(t+h)), \\sin(2\\pi Ut)) \\\\ &amp;= \\mathbb{E}(\\sin(2\\pi U(t+h))\\sin(2\\pi Ut)) \\\\ &amp;= \\int_0^1 \\sin(2\\pi u(t+h))\\sin(2\\pi ut)du \\\\ &amp;= \\frac{1}{2}\\int_0^1 \\left(\\cos(2\\pi uh) + \\cos(2\\pi u(2t+h))\\right)du \\\\ &amp;= \\frac{1}{2}\\left(\\frac{\\sin(2\\pi h)}{2\\pi h} + \\frac{\\sin(2\\pi (h+2t))}{2\\pi (h + 2t)}\\right) \\\\ &amp;= 0 \\end{align*}\\] If \\(h = 0\\), then \\[\\begin{align*} \\gamma(0) &amp;= Cov(x_t, x_t) \\\\ &amp;= \\mathbb{E}(\\sin^2(2\\pi Ut)) \\\\ &amp;= \\int_0^1 \\sin(2\\pi ut)^2du \\\\ &amp;= \\frac{1}{2} - \\frac{\\sin(4\\pi t)}{8\\pi t} \\\\ &amp;= \\frac{1}{2}, \\quad\\text{for}~ t = 1, 2, \\dots \\end{align*}\\] Thus, the serie \\(x_t\\) is weakly stationary. Question (b) We have: \\[\\mathbb{P}(X_1 \\leq c) = \\mathbb{P}(\\sin(2\\pi U) \\leq c) = \\mathbb{P}\\left(U \\leq \\frac{1}{2\\pi}\\sin^{-1}(c)\\right) = \\frac{1}{2\\pi}\\sin^{-1}(c)\\] and \\[\\mathbb{P}(X_2 \\leq c) = \\mathbb{P}(\\sin(4\\pi U) \\leq c) = \\mathbb{P}\\left(U \\leq \\frac{1}{4\\pi}\\sin^{-1}(c)\\right) = \\frac{1}{4\\pi}\\sin^{-1}(c)\\] Thus, \\(\\mathbb{P}(X_1 \\leq c) \\neq \\mathbb{P}(X_2 \\leq c)\\). Finally, the serie \\(x_t\\) is not strictly stationary. 2.1.17 Exercices 1.17 Suppose we have a linear process \\(x_t\\) generated by \\[x_t = w_t - \\theta w_{t-1}, \\quad t = 0, 1, 2, \\dots\\] where \\(\\{w_t\\}\\) is independent and identically distributed with characteristic function \\(\\phi_w(\\cdot)\\) and \\(\\theta\\) a fixed constant. Question (a) \\[\\begin{align*} \\phi_{x_1, \\dots, x_n}(\\lambda_1, \\dots, \\lambda_n) &amp;= \\mathbb{E}(\\exp(i\\langle \\lambda, x \\rangle)) \\\\ &amp;= \\mathbb{E}\\left[\\exp\\left(i\\sum_{j = 1}^n \\lambda_jx_j\\right)\\right] \\\\ &amp;= \\mathbb{E}\\left[\\exp\\left(i\\sum_{j = 1}^n \\lambda_j(w_t - \\theta w_{t-1})\\right)\\right] \\\\ &amp;= \\mathbb{E}\\left[\\exp\\left(i\\sum_{j = 1}^n \\lambda_jw_t\\right)\\right]\\mathbb{E}\\left[\\exp\\left(-i\\sum_{j = 1}^n \\lambda_j\\theta w_{t-1}\\right)\\right] \\\\ &amp;= \\prod_{i = 1}^n \\phi_w(\\lambda_i)\\phi_w(-\\theta\\lambda_i) \\end{align*}\\] Question (b) The characteristic function of \\(x_1, \\dots, x_n\\) is independent of the time, because \\(\\{w\\}\\) is independent and identically distributed. Thus, \\(x_t\\) is strictly stationary. 2.1.18 Exercices 1.18 Let \\(x_t\\) be a linear process of the form: \\[x_t = \\mu + \\sum_{j = -\\infty}^{+\\infty} \\phi_jw_{t-j}, \\quad \\sum_{j = -\\infty}^{+\\infty}\\lvert \\phi_j \\rvert &lt; +\\infty,\\] where \\(\\{w_t\\}\\) is a white noise process. \\[\\begin{align*} \\sum_{h = -\\infty}^{+\\infty}\\lvert \\gamma(h) \\rvert &amp;= \\sum_{h = -\\infty}^{+\\infty}\\left\\lvert \\sigma_w^2 \\sum_{j = -\\infty}^{+\\infty} \\phi_{j+h}\\phi_j\\right\\rvert \\\\ &amp;\\leq \\sigma_w^2\\sum_{j = -\\infty}^{+\\infty}\\sum_{h = -\\infty}^{+\\infty}\\left\\lvert \\phi_{j+h}\\right\\rvert\\left\\lvert\\phi_j \\right\\rvert \\\\ &amp;\\leq \\sigma_w^2\\sum_{j = -\\infty}^{+\\infty}\\left\\lvert\\phi_j \\right\\rvert\\sum_{h = -\\infty}^{+\\infty}\\left\\lvert \\phi_{j+h}\\right\\rvert \\\\ &amp;\\leq +\\infty \\end{align*}\\] 2.1.19 Exercices 1.19 Suppose \\(x_t = \\mu + w_t + \\theta w_{t-1}\\), where \\(w_t \\sim \\mathcal{WN}(0, \\sigma_w^2)\\). Question (a) \\[\\mathbb{E}(x_t) = \\mu + \\mathbb{E}(w_t) + \\theta\\mathbb{E}(w_{t-1}) = \\mu\\] Question (b) \\[\\begin{align*} \\gamma_x(h) &amp;= Cov(x_t, x_{t+h}) \\\\ &amp;= Cov(\\mu + w_t + \\theta w_{t-1}, \\mu + w_{t+h} + \\theta w_{t+h-1}) \\\\ &amp;= Cov(w_t, w_{t+h}) + \\theta Cov(w_t, w_{t+h-1}) + \\theta Cov(w_{t-1}, w_{t+h}) + \\theta^2Cov(w_{t-1}, w_{t+h-1}) \\\\ &amp;= \\left\\{ \\begin{array}{c l l} (1 + \\theta^2)\\sigma_w^2 &amp;\\text{if}&amp; h = 0\\\\ \\theta \\sigma_w^2 &amp;\\text{if}&amp; \\lvert h \\rvert = 1 \\\\ 0 &amp;\\text{else} \\end{array} \\right. \\end{align*}\\] Question (c) For all values of \\(\\theta \\in \\mathbb{R}\\), \\(\\mathbb{E}(x_t) = \\mu\\) is independent of \\(t\\) and \\(\\gamma_x(h)\\) depends only on \\(h\\). So, \\(x_t\\) is stationary. Question (d) \\[\\begin{align*} Var(\\bar{x}) &amp;= \\frac{1}{n}\\sum_{h=-n}^n \\left(1 - \\frac{\\lvert h \\rvert}{n}\\right)\\gamma_x(h) \\\\ &amp;= \\frac{1}{n}\\left(2\\left(1 - \\frac{1}{n}\\right)\\theta \\sigma_w^2 + (1 + \\theta^2)\\sigma_w^2\\right)\\\\ &amp;= \\frac{\\sigma_w^2}{n}\\left(2\\left(1 - \\frac{1}{n}\\right)\\theta + (1 + \\theta^2)\\right) \\\\ \\end{align*}\\] If \\(\\theta = 1\\), \\(Var(\\bar{x}) = \\frac{\\sigma_w^2}{n}\\left(4 - \\frac{2}{n}\\right)\\). If \\(\\theta = 0\\), \\(Var(\\bar{x}) = \\frac{\\sigma_w^2}{n}\\). If \\(\\theta = -1\\), \\(Var(\\bar{x}) = \\frac{2\\sigma_w^2}{n^2}\\). Question (e) The standard error of the estimate of the mean is the square root of \\(Var(\\bar{x})\\). If \\(\\theta = 1\\), \\(Var(\\bar{x}) \\approx \\frac{4\\sigma_w^2}{n} = \\mathcal{o}(1)\\). If \\(\\theta = 0\\), \\(Var(\\bar{x}) = \\frac{\\sigma_w^2}{n} = \\mathcal{o}(1)\\). If \\(\\theta = -1\\), \\(Var(\\bar{x}) \\approx 0\\). 2.1.20 Exercices 1.20 Question (a) w = rand(Normal(), 500); # Plot the ACF figure(figsize=(15, 5)) stem(0:20, autocor(w, 0:20), use_line_collection=true) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() Question (b) w = rand(Normal(), 50); # Plot the ACF figure(figsize=(15, 5)) stem(0:20, autocor(w, 0:20), use_line_collection=true) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() The changing of \\(n\\) affects a lot the results, when \\(n = 500\\) the ACF has converged yet while it’s not the case when \\(n = 50\\). 2.1.21 Exercices 1.21 Question (a) w = rand(Normal(), 550); v = filter_convolution(w, [1/3, 1/3, 1/3], 2)[50:549]; # Plot the ACF figure(figsize=(15, 5)) stem(0:20, autocor(v, 0:20), use_line_collection=true) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() w = rand(Normal(), 100); v = filter_convolution(w, [1/3, 1/3, 1/3], 2)[50:99]; # Plot the ACF figure(figsize=(15, 5)) stem(0:20, autocor(v, 0:20), use_line_collection=true) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() The changing of \\(n\\) affects a bit the results. When \\(n = 50\\), it looks like there is a periodic trend in the serie, while when \\(n = 500\\), this trend tends to disappear and the ACF has converged. 2.1.22 Exercices 1.22 Let’s assume the following signal-plus-noise model: \\[x_t = s_t + w_t, \\quad w_t \\sim \\mathcal{N}(0, 1)\\] \\[s_t = \\left\\{ \\begin{array}{l l} 0, &amp; t = 1, \\dots, 100 \\\\ 10\\exp\\left(-\\frac{(t - 100)}{20}\\right)\\cos(2\\pi t / 4), &amp; t = 101, \\dots, 200 \\end{array} \\right.\\] x = signal_plus_noise_a(1:200); # Plot the ACF figure(figsize=(15, 5)) stem(0:23, autocor(x), use_line_collection=true) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() We see in the autocorrelation function the exponential decresing of the serie (the \\(\\exp\\left(-t\\right)\\) part. Every even \\(h\\), the ACF is equal to \\(0\\) because of the \\(\\cos(2\\pi t)\\) part. 2.1.23 Exercices 1.23 Consider the model \\[x_t = 2\\cos\\left(2\\pi \\frac{t + 15}{50}\\right) + w_t,\\] for \\(t = 1, \\dots, 500\\), \\(\\{w_t\\}\\) is a white noise with \\(\\sigma_w^2 = 1\\). function signal_c(l) return [2 * cos(2 * pi * (t + 15) / 50) for t in l] end function signal_plus_noise_c(l) w = rand(Normal(), length(l)) return signal_c(l) + w end signal_plus_noise_c (generic function with 1 method) x = signal_plus_noise_c(1:500); figure(figsize=(15, 5)) plot(1:500, x) xlabel(L&quot;$t$&quot;) ylabel(L&quot;$x_t$&quot;) title(L&quot;x_t = s_t + w_t&quot;) show() # Plot the ACF figure(figsize=(15, 5)) stem(0:100, autocor(x, 0:100), use_line_collection=true) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() We see the periodic components of the serie in the ACF. 2.1.24 Exercices 1.24 Let \\(X\\) be a latent variable representing the action of tossing a fair coin. We have that: \\[\\mathbb{P}(X = head) = \\mathbb{P}(X = tail) = 0.5.\\] Let \\(x_t\\) be a time serie, such that \\(x_t = 1\\) if \\(X = head\\) and \\(x_t = -1\\) if \\(X = tail\\). Then, we construct \\(y_t\\) as \\[y_t = 5 + x_t - 0.7x_{t-1}.\\] The moment of \\(X\\) are: \\[\\begin{align*} \\mathbb{E}(X) &amp;= \\mathbb{P}(X = head) \\times 1 + \\mathbb{P}(X = tail) \\times (-1) = 0 \\\\ Var(X) &amp;= \\mathbb{E}(X^2) = \\mathbb{P}(X = head) \\times 1^2 + \\mathbb{P}(X = tail) \\times (-1)^2 = 1 \\\\ \\end{align*}\\] The autocovariance function of \\(y_t\\) is \\[\\begin{align*} \\gamma_y(h) &amp;= Cov(5 + x_{t+h} - 0.7x_{t+h-1}, 5 + x_t - 0.7x_{t-1}) \\\\ &amp;= Cov(x_{t+h}, x_t) - 0.7Cov(x_{t+h}, x_{t-1}) - 0.7Cov(x_{t+h-1}, x_t) + 0.7^2Cov(x_{t+h-1}, x_{t-1}) \\\\ &amp;= \\left\\{ \\begin{array}{l l} 1 + 0.7^2 &amp;\\text{if} h = 0 \\\\ -0.7 &amp;\\text{if} h = 1 \\\\ 0 &amp;\\text{else} \\end{array} \\right. \\end{align*}\\] Finally, we have that \\[\\rho_y(1) = \\frac{-0.7}{1 + 0.7^2} \\quad\\text{and}\\quad \\rho_y(h) = 0, \\quad\\text{if}\\quad h &gt; 1.\\] 2.1.25 Exercices 1.25 Question (a) Let show that the autocovariance function \\(\\gamma(h)\\) of any stationary time series \\(x_t\\) is non-negative definite function. Let \\(a = (a_1, \\dots, a_n)^\\top \\in \\mathbb{R}^n\\), \\(t = (t_1, \\dots, t_n)^\\top \\in \\mathbb{Z}^n\\) and \\(Z_t = (X_{t_1} - \\mathbb{E}(X_{t_1}), \\dots, X_{t_n} - \\mathbb{E}(X_{t_n}))\\). Then, \\[\\begin{align*} 0 &amp;\\leq Var(a^\\top Z_t) \\\\ &amp;= a^\\top \\mathbb{E}(Z_tZ_t^\\top)a \\\\ &amp;= a^\\top \\Gamma_n a \\\\ &amp;= \\sum_{i = 1}^n \\sum_{j=1}^n a_i \\gamma(t_i - t_j) a_j \\end{align*}\\] where \\(\\Gamma_n\\) is the covariance matrix of \\((X_{t_1}, \\dots, X_{t_n})\\). Question (b) Assume, without loss of generality, that \\(\\bar{x} = 0\\). Let \\(x_t = 0\\) for \\(t &lt; 1\\) or \\(t &gt; n\\) and define the column vector \\(X_i = (x_{i+1}, \\dots, x_{i+n})^\\top, -n &lt; i &lt; n\\). Then \\[n\\Gamma_n = \\sum_{i = 1 - n}^{n - 1}X_iX_i^\\top,\\] and \\(a^\\top X_iX_i^\\top a = (X_i^\\top a)^\\top(X_i^\\top)a \\geq 0\\) for any column vector \\(a\\). Thus, \\(\\Gamma_n\\) is the sum of nonnegative definite matrices and consequently must also be nonnegative definite. It follows that \\(\\hat{\\gamma}(h)\\) is nonnegative definite for any \\(h &gt; 0\\). 2.1.26 Exercices 1.26 Consider a collection of time series \\(x_{1t}, x_{2t}, \\dots, x_{Nt}\\) that are observing some common signal \\(\\mu_t\\) observed in noise processes \\(e_{1t}, e_{2t}, \\dots, e_{Nt}\\), with a model for the \\(j\\)th observed series given by \\[x_{jt} = \\mu_t + e_{jt}.\\] Suppose the noise series have zero means and are uncorrelated for different \\(j\\). The common autocovariance functions of all series are given by \\(\\gamma_e(s, t)\\). Define the sample mean \\[\\bar{x}_t = \\frac{1}{N}\\sum_{j=1}^N x_{jt}.\\] Question (a) \\[\\mathbb{E}(\\bar{x}_t) = \\mathbb{E}\\left(\\frac{1}{N}\\sum_{j=1}^N x_{jt}\\right) = \\mathbb{E}\\left(\\frac{1}{N}\\sum_{j=1}^N (\\mu_t + e_{jt})\\right) = \\frac{1}{N}\\sum_{j=1}^N \\mu_t = \\mu_t\\] Question (b) \\[\\begin{align*} \\mathbb{E}\\left((\\bar{x}_t - \\mu_t)^2\\right) &amp;= \\mathbb{E}\\left(\\left(\\frac{1}{N}\\sum_{j=1}^N (\\mu_t + e_{jt}) - \\mu_t\\right)^2\\right) \\\\ &amp;= \\mathbb{E}\\left(\\left(\\frac{1}{N}\\sum_{j=1}^N e_{jt}\\right)^2\\right) \\\\ &amp;= \\frac{1}{N^2}\\mathbb{E}\\left(\\left(\\sum_{j=1}^N e_{jt}\\right)^2\\right) \\\\ &amp;= \\frac{1}{N}\\gamma_e(t, t), \\quad\\text{because the noise series are uncorrelated}. \\end{align*}\\] Question (c) The common signal may be estimated using \\(\\bar{x}_t\\), because the square difference between \\(\\bar{x}_t\\) and the common signal goes to zero (in mean) as \\(N\\) goes to infinity. 2.1.27 Exercices 1.27 Let \\(x_s, s = (s_1, s_2)\\) be a spatial process. For \\(s_1, s_2 = 0, \\pm 1, \\pm 2, \\dots\\), defined the variogram \\[V_x(h) = \\frac{1}{2}\\mathbb{E}((x_{s+h} - x_s)^2)\\] where \\(h = (h_1, h_2)\\), for \\(h_1, h_2 = 0, \\pm 1, \\pm 2, \\dots\\). We have that \\[\\begin{align*} V_x(h) &amp;= \\frac{1}{2}\\mathbb{E}((x_{s+h} - x_s)^2) \\\\ &amp;= \\frac{1}{2}\\mathbb{E}(((x_{s+h} - \\mu) - (x_s - \\mu))^2) \\\\ &amp;= \\frac{1}{2}\\mathbb{E}((x_{s+h} - \\mu)^2 + (x_s - \\mu)^2 - 2(x_{s+h} - \\mu)(x_s - \\mu)) \\\\ &amp;= \\frac{1}{2}\\left(\\mathbb{E}((x_{s+h} - \\mu)^2) + \\mathbb{E}((x_s - \\mu)^2) - 2\\mathbb{E}((x_{s+h} - \\mu)(x_s - \\mu))\\right) \\\\ &amp;= \\gamma(0) - \\gamma(h) \\end{align*}\\] 2.1.28 Exercices 1.28 Suppose \\(x_t = \\beta_0 + \\beta_1t\\), where \\(\\beta_0\\) ad \\(\\beta_1\\) are constants. We will prove that, as \\(n \\rightarrow \\infty\\), \\(\\hat{\\rho}_x(h) \\rightarrow 1\\), for fixed \\(h\\). \\[\\begin{align*} \\bar{x} &amp;= \\frac{1}{n}\\sum_{t = 1}^n (\\beta_0 + \\beta_1t) = \\beta_0 + \\beta_1\\frac{n(n+1)}{2} \\\\ \\hat{\\rho}_x(h) &amp;= \\frac{\\sum_{t=1}^{n-h}(x_{t+h} - \\bar{x})(x_t - \\bar{x})}{\\sum_{t=1}^{n}(x_{t} - \\bar{x})(x_t - \\bar{x})} \\\\ \\end{align*}\\] Then, \\[\\begin{align*} \\sum_{t=1}^{n}(x_{t} - \\bar{x})(x_t - \\bar{x}) &amp;= \\beta_1^2\\sum_{t=1}^n\\left(t - \\frac{n(n+1)}{2}\\right)^2 \\\\ &amp;= \\beta_1^2\\sum_{t=1}^n\\left(t^2 - 2t\\frac{n(n+1)}{4} + \\frac{n^2(n+1)^2}{4}\\right) \\\\ &amp;= \\beta_1^2\\left(\\frac{n(n+1)(2n+1)}{6} - \\frac{n^2(n+1)^2}{2} + \\frac{n^3(n+1)^2}{4}\\right) \\\\ &amp;= \\frac{\\beta_1^2}{12}\\left(2n(n+1)(2n+1) - 6n^2(n+1)^2 + 3n^3(n+1)^2\\right) \\\\ &amp;= \\mathcal{O}(n^5) \\end{align*}\\] \\[\\begin{align*} \\sum_{t=1}^{n-h}(x_{t+h} - \\bar{x})(x_t - \\bar{x}) &amp;= \\beta_1^2\\sum_{t=1}^n\\left(t + h - \\frac{n(n+1)}{2}\\right)\\left(t - \\frac{n(n+1)}{2}\\right) \\\\ &amp;= \\beta_1^2\\sum_{t=1}^n\\left(t^2 - tn(n+1) + \\frac{n^2(n+1)^2}{4} + ht - h\\frac{n(n+1)}{2}\\right) \\\\ &amp;= \\beta_1^2\\left(\\frac{n(n+1)(2n+1)}{6} - \\frac{n^2(n+1)^2}{2} + \\frac{n^3(n+1)^2}{4} + h\\frac{n(n+1)}{4} - h\\frac{n^2(n+1)^2}{4}\\right) \\\\ &amp;= \\frac{\\beta_1^2}{12}\\left(2n(n+1)(2n+1) - 6n^2(n+1)^2 + 3n^3(n+1)^2 + 6hn(n+1)(1-n)\\right) \\\\ &amp;= \\mathcal{O}(n^5) \\end{align*}\\] Finally, \\(\\hat{\\rho}_x(h) = \\mathcal{O}(1)\\), as \\(n \\rightarrow \\infty\\). 2.1.29 Exercices 1.29 Question (a) Suppose \\(x_t\\) is a weakly stationary time series with mean zero and with absolutely summable autocovariance function, \\(\\gamma(h)\\), such that \\[\\sum_{h=-\\infty}^{\\infty} \\gamma(h) = 0\\] The variance of the sample mean \\(\\bar{x}\\) is (see e.g. J. Hamilton,Time Series Analysis, 1994, p.187-188), \\[Var(\\bar{x}) = \\frac{1}{n^2}\\sum_{h = -n}^n (n - \\lvert h \\rvert)\\gamma(h) = \\frac{1}{n}\\sum_{h = -n}^n \\frac{n - \\lvert h \\rvert}{n}\\gamma(h).\\] And then, \\[nVar(\\bar{x}) = \\sum_{h = -n}^n \\frac{n - \\lvert h \\rvert}{n}\\gamma(h) \\underset{n \\rightarrow \\infty}{\\longrightarrow} \\sum_{h = -\\infty}^\\infty \\gamma(h) = 0,\\] by dominated convergence, because \\(\\lvert (1 - \\lvert h\\rvert / n)\\gamma(h)\\rvert \\leq \\lvert\\gamma(h)\\rvert\\) and \\((1 - \\lvert h\\rvert / n)\\gamma(h) \\rightarrow \\gamma(h)\\). Finally, \\(\\sqrt{n}\\bar{x} \\overset{ms}{\\rightarrow} 0\\), which implies \\(\\sqrt{n}\\bar{x} \\overset{p}{\\rightarrow} 0\\). Question (b) I think that the only process that satisfies the previous assumption in a constant process with mean \\(0\\), because, in order to have \\(\\sum_{h = -\\infty}^\\infty \\gamma(h) = 0\\), the variance of the process should be \\(0\\) and only deterministic processes have null variance. Moreover, it should have zero mean. And, for a deterministic process, only constant ones have mean that does not depend on time. 2.1.30 Exercices 1.30 Let \\(x_t\\) be a linear process of the form \\[x_t = \\mu_x + \\sum_{j = -\\infty}^{\\infty} \\psi_j w_{t-j}\\] where \\(w_t\\) is a white noise process with variance \\(\\sigma^2\\), and \\(\\sum \\lvert \\psi_j \\rvert &lt; \\infty\\). Define \\[\\begin{align*} \\hat \\gamma \\left( h \\right) &amp;= \\frac{1}{n}\\sum_{t = 1}^{n - h} {\\left( x_t - \\bar x \\right)\\left( x_{t + h} - \\bar x \\right)}\\\\ \\tilde{\\gamma} \\left( h \\right) &amp;= \\frac{1}{n}\\sum_{t = 1}^{n} {\\left( x_t - \\mu_x \\right)\\left( x_{t + h} - \\mu_x \\right)} \\end{align*}\\] We want to show that \\[n^{\\frac{1}{2}}\\left(\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)\\right) = \\mathcal{o}_p(1) \\Leftrightarrow \\mathbb{P}\\left(\\left\\lvert n^{\\frac{1}{2}}\\left(\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)\\right)\\right\\rvert &gt; \\epsilon \\right) \\longrightarrow 0, \\forall \\epsilon &gt; 0.\\] Using the Markov inequality, we have that: \\[\\forall \\epsilon &gt; 0, \\mathbb{P}\\left(\\left\\lvert n^{\\frac{1}{2}}\\left(\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)\\right)\\right\\rvert &gt; \\epsilon\\right) \\leq \\frac{1}{\\epsilon}\\mathbb{E}\\left(\\left\\lvert n^{\\frac{1}{2}}\\left(\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)\\right)\\right\\rvert\\right)\\] Then, \\[\\begin{align*} n^{\\frac{1}{2}}\\left(\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)\\right) &amp;= n^{\\frac{1}{2}}\\left(\\frac{1}{n}\\sum_{t = 1}^{n} {\\left( x_t - \\mu_x \\right)\\left( x_{t + h} - \\mu_x \\right)} - \\frac{1}{n}\\sum_{t = 1}^{n - h} {\\left( x_t - \\bar x \\right)\\left( x_{t + h} - \\bar x \\right)}\\right) \\\\ &amp;= \\frac{1}{n^{\\frac{1}{2}}}\\left(\\sum_{t = n-h+1}^{n} {\\left( x_t - \\mu_x \\right)\\left( x_{t + h} - \\mu_x \\right)} + \\sum_{t = 1}^{n - h} {\\left( x_t - \\mu_x \\right)\\left( x_{t + h} - \\mu_x \\right) - \\left( x_t - \\bar x \\right)\\left( x_{t + h} - \\bar x \\right)}\\right) \\end{align*}\\] Considering only the second term, \\[\\begin{align*} \\sum_{t = 1}^{n - h} {\\left( x_t - \\mu_x \\right)\\left( x_{t + h} - \\mu_x \\right) - \\left( x_t - \\bar x \\right)\\left( x_{t + h} - \\bar x \\right)} &amp;= \\sum_{t = 1}^{n - h} \\left({x_t (\\bar x - \\mu_x) + x_{t + h} (\\bar x - \\mu_x) - \\left( \\bar x - \\mu_x\\right)\\left( \\bar x + \\mu_x\\right)}\\right)\\\\ &amp;= \\left(\\bar x - \\mu_x\\right)\\sum_{t = 1}^{n - h} \\left({x_t + x_{t + h} - \\bar x - \\mu_x}\\right) \\\\ &amp;= \\left(\\bar x - \\mu_x\\right)\\left(\\sum_{t = 1}^{n - h} x_t + \\sum_{t = 1+h}^{n} x_{t} - (n-h)\\bar x - (n-h)\\mu_x\\right) \\\\ &amp;= \\left(\\bar x - \\mu_x\\right)\\left(\\sum_{t = 1+h}^{n - h} x_t + \\sum_{t = 1}^{n} x_{t} - (n-h)\\bar x - (n-h)\\mu_x\\right) \\\\ &amp;= \\left(\\bar x - \\mu_x\\right)\\left(\\sum_{t = 1+h}^{n - h} x_t + h\\bar x - (n-h)\\mu_x\\right) \\\\ &amp;= \\left(\\bar x - \\mu_x\\right)\\left(\\sum_{t = 1+h}^{n - h} (x_t - \\mu_x) + h(\\bar x - \\mu_x)\\right) \\\\ &amp;= \\left(\\bar x - \\mu_x\\right)\\sum_{t = 1+h}^{n - h} (x_t - \\mu_x) + h(\\bar x - \\mu_x)^2 \\\\ \\end{align*}\\] Returning to the previous equation, we have that \\[n^{\\frac{1}{2}}\\left(\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)\\right) = \\frac{1}{n^{\\frac{1}{2}}}\\sum_{t = n-h+1}^{n} {\\left( x_t - \\mu_x \\right)\\left( x_{t + h} - \\mu_x \\right)} + \\frac{1}{n^{\\frac{1}{2}}}\\left(\\bar x - \\mu_x\\right)\\sum_{t = 1+h}^{n - h} (x_t - \\mu_x) + \\frac{h}{n^{\\frac{1}{2}}}(\\bar x - \\mu_x)^2 \\] We continue using the triangle inequality: \\[\\mathbb{E}\\left(\\left\\lvert n^{\\frac{1}{2}}\\left(\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)\\right)\\right\\rvert\\right) \\leq \\frac{1}{n^{\\frac{1}{2}}}\\sum_{t = n-h+1}^{n} \\mathbb{E}{\\left\\lvert( x_t - \\mu_x )( x_{t + h} - \\mu_x )\\right\\rvert} + \\frac{1}{n^{\\frac{1}{2}}}\\mathbb{E}\\left[\\left\\lvert\\left(\\bar x - \\mu_x\\right)\\sum_{t = 1+h}^{n - h} (x_t - \\mu_x)\\right\\rvert\\right] + \\frac{h}{n^{\\frac{1}{2}}}\\mathbb{E}(\\bar x - \\mu_x)^2.\\] For the first term, using Cauchy-Schwarz inequality, we have: \\[\\mathbb{E}{\\left\\lvert( x_t - \\mu_x )( x_{t + h} - \\mu_x )\\right\\rvert} \\leq \\mathbb{E}\\left(\\left\\lvert( x_t - \\mu_x )\\right\\rvert^2\\right)^{1/2} \\mathbb{E}\\left(\\left\\lvert( x_{t + h} - \\mu_x )\\right\\rvert^2\\right)^{1/2} = \\sigma^2\\sum_{j=-\\infty}^{\\infty} \\psi_j^2 \\leq \\sigma^2\\left(\\sum_{j=-\\infty}^{\\infty} \\lvert\\psi_j\\rvert\\right)^2&lt; \\infty\\] For the last term, using equation (A.46)-(A-47), we have: \\[\\mathbb{E}(\\bar x - \\mu_x)^2 = \\frac{\\sigma^2}{n}\\left(\\sum_{j=-\\infty}^{\\infty} \\psi_j\\right)^2 \\leq \\frac{\\sigma^2}{n}\\left(\\sum_{j=-\\infty}^{\\infty} \\lvert\\psi_j\\rvert\\right)^2 &lt; \\infty\\] Finally, for the middle term, again using Cauchy-Schwarz inequality, we have: \\[\\begin{align*} \\mathbb{E}\\left[\\left\\lvert\\left(\\bar x - \\mu_x\\right)\\sum_{t = 1+h}^{n - h} (x_t - \\mu_x)\\right\\rvert\\right] &amp;\\leq \\mathbb{E}\\left[\\left\\lvert\\left(\\bar x - \\mu_x\\right)\\right\\rvert^2\\right]^{1/2}\\mathbb{E}\\left[\\left\\lvert\\sum_{t = 1+h}^{n - h} (x_t - \\mu_x)\\right\\rvert^2\\right]^{1/2} \\\\ &amp;\\leq \\frac{\\sigma}{\\sqrt{n}}\\left(\\sum_{j=-\\infty}^{\\infty} \\lvert\\psi_j\\rvert\\right)\\left[\\sum_{t = 1+h}^{n - h} \\mathbb{E}(x_t - \\mu_x)^2\\right]^{1/2} \\\\ &amp;\\leq \\frac{\\sigma}{\\sqrt{n}}\\left(\\sum_{j=-\\infty}^{\\infty} \\lvert\\psi_j\\rvert\\right)\\left[\\sum_{t = 1+h}^{n - h} Var(x_t)\\right]^{1/2} \\\\ &amp;\\leq \\frac{\\sigma}{\\sqrt{n}}\\left(\\sum_{j=-\\infty}^{\\infty} \\lvert\\psi_j\\rvert\\right)(n-2h)^{1/2}\\sigma\\left(\\sum_{j=-\\infty}^{\\infty} \\psi_j^2\\right)^{1/2} \\\\ &amp;\\leq \\sqrt{\\frac{n-2h}{n}}\\sigma^2\\left(\\sum_{j=-\\infty}^{\\infty} \\lvert\\psi_j\\rvert\\right)^2 &lt; \\infty \\end{align*}\\] To conclude the proof, we write \\[\\mathbb{E}\\left(\\left\\lvert n^{\\frac{1}{2}}\\left(\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)\\right)\\right\\rvert\\right) \\leq \\frac{nh + \\sqrt{n(n - 2h)} + h}{n\\sqrt{n}}\\sigma^2\\left(\\sum_{j=-\\infty}^{\\infty} \\lvert\\psi_j\\rvert\\right)^2 \\longrightarrow 0.\\] 2.1.31 Exercices 1.31 Let \\(x_t\\) be a linear process of the form \\[x_t = \\sum_{j=0}^\\infty \\phi^j w_{t-j}\\] where \\(\\{w_t\\}\\) as finite fourth moment and \\(\\lvert \\phi \\rvert &lt; 1\\). In particular, \\(x_t\\) is a process of the form \\[x_t = \\sum_{j=0}^\\infty \\psi_j w_{t-j} \\quad\\text{where}\\quad \\psi_j = \\left\\{\\begin{array}{r l} \\phi^j, &amp;\\quad j \\geq 0 \\\\ 0, &amp;\\quad j &lt; 0 \\end{array}\\right.\\] Moreover, \\(x_t\\) may be written under an AR(1) form \\[x_t = \\phi X_{t-1} + w_t.\\] The autocovariance function of an AR(1) process is given by \\[\\gamma(h) = \\sigma^2\\sum_{j=0}^\\infty \\phi^j \\phi^{j+h} = \\sigma^2 \\phi^h\\sum_{j=0}^\\infty \\phi^{2j} = \\frac{\\sigma^2\\phi^h}{1 - \\phi^2}.\\] And then, the variance of an AR(1) process is \\[\\gamma(0) = \\frac{\\sigma^2}{1 - \\phi^2},\\] and its autocorrelation function is \\[\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\phi^h.\\] As, \\(x_t\\) satifies the assumptions of the theorem A.7, we have the following result: \\[\\widehat{\\rho}(1) \\sim A\\mathcal{N}(\\rho(1), n^{-1}W),\\] where \\(W = \\sum_{u=1}^\\infty \\left(\\rho(u+1) + \\rho(u-1) - 2\\rho(1)\\rho(u)\\right)^2.\\) Let’s compute \\(W\\): \\[\\begin{align*} W &amp;= \\sum_{u=1}^\\infty \\left(\\rho(u+1) + \\rho(u-1) - 2\\rho(1)\\rho(u)\\right)^2 \\\\ &amp;= \\sum_{u=1}^\\infty \\left(\\phi^{u+1} + \\phi^{u-1} - 2\\phi^1\\phi^u\\right)^2 \\\\ &amp;= \\sum_{u=1}^\\infty \\left(\\phi^{u-1} - \\phi^{u+1}\\right)^2 \\\\ &amp;= \\sum_{u=1}^\\infty \\left(\\phi^{2(u-1)} + \\phi^{2(u+1)} - 2\\phi^{(u+1) + (u+1)}\\right)^2 \\\\ &amp;= \\sum_{u=1}^\\infty \\left(\\phi^{2u}\\left(\\phi^2 + \\phi^{-2} - 2\\right)\\right) \\\\ &amp;= \\left(\\phi^2 + \\phi^{-2} - 2\\right)\\left(\\sum_{u=0}^\\infty \\phi^{2u} - 1\\right) \\\\ &amp;= \\left(\\phi^2 + \\phi^{-2} - 2\\right)\\left(\\frac{1}{1 - \\phi^2} - 1\\right) \\\\ &amp;= \\frac{\\phi^4 - 2\\phi^2 + 1}{1 - \\phi^2} \\\\ &amp;= 1 - \\phi^2 \\\\ &amp;= 1 - \\rho(1)^2 \\end{align*}\\] Finally, we have the following convergence in distribution: \\[\\sqrt{n}\\frac{\\widehat{\\rho}(1) - \\rho(1)}{\\sqrt{1 - \\rho(1)^2}} \\xrightarrow{d} \\mathcal{N}(0, 1).\\] A confidence for \\(\\rho(1)\\) (and so for \\(\\phi\\) is given by \\[\\left[\\widehat{\\rho}(1) - \\mathcal{t}_{0.975, n-1}\\sqrt{\\frac{1 - \\widehat{\\rho}^2(1)}{n}}; \\widehat{\\rho}(1) + \\mathcal{t}_{0.975, n-1}\\sqrt{\\frac{1 - \\widehat{\\rho}^2(1)}{n}}\\right] = [0.49; 0.79],\\] where \\(\\mathcal{t}_{0.975, n-1}\\) is the quantile of the student distribution at confidence level \\(97.5\\%\\) and \\(n-1\\) degree of freedom. We use the student distribution, because we had to estimate the standard deviation of \\(\\rho(1)\\). 2.1.32 Exercices 1.32 Let \\(\\{x_t, t = 0, \\pm 1, \\pm 2, \\dots\\}\\) be iid\\((0, \\sigma^2)\\). Question (a) Let \\(h \\geq 1\\), \\(k \\geq 1\\) and \\(s \\neq t\\), then \\[ Cov(x_tx_{t+h}, x_sx_{s+k}) = \\mathbb{E}(x_tx_{t+h}x_sx_{s+k}) + \\mathbb{E}(x_tx_{t+h})\\mathbb{E}(x_sx_{s+k}) = 0, \\] Question (b) Let \\(c = (c_1, \\dots, c_h)^\\top \\in \\mathbb{R}^h\\), and \\(X_n = (\\sum x_tx_{t+1}, \\dots, \\sum x_tx_{t+h})^\\top\\). We aim to compute the moments of \\(c^\\top X_n\\) \\[\\begin{align*} \\mathbb{E}(c^\\top X_n) &amp;= \\sum_{i = 1}^h c_i \\sum_{t=1}^n \\mathbb{E}(x_t x_{t+i}) = 0 \\\\ Var(c^\\top X_n) &amp;= \\sum_{i = 1}^h c_i^2 \\sum_{t=1}^n Var(x_t x_{t+i}) = n\\sigma^4 \\sum_{i=1}^h c_i^2 \\end{align*}\\] because, we have for two random variables \\(X\\) and \\(Y\\): \\[Var(XY) = \\mathbb{E}(X^{2}Y^{2}) - \\mathbb{E}(XY)^{2} = Cov(X^{2}, Y^{2}) + [Var(X) + \\mathbb{E}(X)^2] \\times [Var(Y) + \\mathbb{E}(Y)^2] - [ Cov(X,Y) + \\mathbb{E}(X)\\mathbb{E}(Y) ]^{2}\\] So, we have the following result: \\[\\sigma^{-2}n^{-1/2}c^\\top X_n \\xrightarrow{d} c^\\top z, \\quad\\text{where}\\quad z = (z_1, \\dots, z_h)^\\top\\] with \\(z_1, \\dots, z_h\\) are iid \\(\\mathcal{N}(0, 1)\\) random variables. Finally, using the Cramér-Wold device, \\[\\sigma^{-2}n^{-1/2} X_n \\xrightarrow{d} z\\] Question (c) We would like to show that, for each \\(h \\geq 1\\), \\[n^{-1/2}\\left(\\sum_{t=1}^{n} x_tx_{t+h} - \\sum_{t=1}^{n-h}(x_t - \\bar x)(x_{t+h} - \\bar x)\\right) \\xrightarrow{p} 0, \\quad\\text{as}~ n \\rightarrow \\infty\\] This is equivalent to show that \\[\\mathbb{P}\\left(\\left\\lvert n^{-1/2}\\left(\\sum_{t=1}^{n} x_tx_{t+h} - \\sum_{t=1}^{n-h}(x_t - \\bar x)(x_{t+h} - \\bar x)\\right)\\right\\rvert &gt; \\epsilon\\right) \\xrightarrow[n \\rightarrow \\infty]{} 0, \\forall \\epsilon &gt; 0.\\] Then, using the Markov inequality, \\[\\begin{align*} \\mathbb{P}\\left(\\left\\lvert n^{-1/2}\\left(\\sum_{t=1}^{n} x_tx_{t+h} - \\sum_{t=1}^{n-h}(x_t - \\bar x)(x_{t+h} - \\bar x)\\right)\\right\\rvert &gt; \\epsilon\\right) &amp;\\leq \\frac{1}{\\epsilon}\\mathbb{E}\\left(\\left\\lvert n^{-1/2}\\left(\\sum_{t=1}^{n} x_tx_{t+h} - \\sum_{t=1}^{n-h}(x_t - \\bar x)(x_{t+h} - \\bar x)\\right)\\right\\rvert\\right) \\\\ &amp;\\leq \\frac{1}{\\epsilon\\sqrt{n}}\\mathbb{E}\\left(\\left\\lvert \\left(\\sum_{t=n-h+1}^{n} x_tx_{t+h} + \\bar x \\sum_{t=1}^{n-h}(x_t + x_{t+h}) - (n-h)\\bar x^2\\right)\\right\\rvert\\right) \\\\ &amp;\\leq \\frac{1}{\\epsilon\\sqrt{n}} \\mathbb{E}\\left(\\left\\lvert\\sum_{t=n-h+1}^{n} x_tx_{t+h}\\right\\rvert\\right) + \\mathbb{E}\\left(\\left\\lvert\\bar x \\sum_{t=1}^{n-h}(x_t + x_{t+h})\\right\\rvert\\right) + (n-h)\\mathbb{E}\\left(\\bar x^2\\right) \\\\ \\end{align*}\\] First term: \\[ \\mathbb{E}\\left(\\left\\lvert\\sum_{t=n-h+1}^{n} x_tx_{t+h}\\right\\rvert\\right) \\leq \\sum_{t=n-h+1}^{n}\\mathbb{E}\\left\\lvert x_tx_{t+h}\\right\\rvert \\leq \\sum_{t=n-h+1}^{n}\\mathbb{E}(x_t^2)^{1/2}\\mathbb{E}(x_{t+h}^2)^{1/2} \\leq h\\sigma^2\\] Last term: \\[\\mathbb{E}\\left(\\bar x^2\\right) = Var(\\bar x^2) = \\frac{1}{n^2}\\sum_{t=1}^n Var(x_t) = \\frac{\\sigma^2}{n} \\] Middle term: \\[\\mathbb{E}\\left(\\left\\lvert\\bar x \\sum_{t=1}^{n-h}(x_t + x_{t+h})\\right\\rvert\\right) \\leq 2\\mathbb{E}\\left(\\left\\lvert\\bar x \\sum_{t=1}^{n-h} x_t\\right\\rvert\\right) \\leq \\mathbb{E}(\\bar x^2)^{1/2}\\mathbb{E}\\left(\\left\\lvert\\sum_{t=1}^{n-h} x_t\\right\\rvert^2\\right)^{1/2} \\leq \\frac{2\\sigma}{\\sqrt{n}}\\left(\\sum_{t=1}^{n-h} \\mathbb{E}(x_t^2)\\right)^{1/2} \\leq 2\\sqrt{\\frac{n - h}{n}}\\sigma^2\\] And finally, \\[\\mathbb{P}\\left(\\left\\lvert n^{-1/2}\\left(\\sum_{t=1}^{n} x_tx_{t+h} - \\sum_{t=1}^{n-h}(x_t - \\bar x)(x_{t+h} - \\bar x)\\right)\\right\\rvert &gt; \\epsilon\\right) \\leq \\frac{\\sigma^2}{\\epsilon}\\frac{nh + 2\\sqrt{n(n-h} + 1}{n\\sqrt{n}} \\xrightarrow[n \\rightarrow \\infty]{} 0.\\] Question (d) Noting that \\(n^{-1}\\sum_{t=1}^n x_t^2 \\xrightarrow{p} \\sigma^2\\), and let \\(\\widehat\\rho = (\\widehat\\rho(1), \\dots, \\widehat\\rho(h))^\\top\\). We conclude that \\[n^{1/2}\\widehat\\rho \\xrightarrow{p} \\sigma^{-2}n^{-1/2}X_n \\xrightarrow{d} z \\Longleftrightarrow n^{1/2}\\widehat\\rho \\xrightarrow{d} z.\\] "],["time-series-regression-and-exploratory-data-analysis.html", "Chapter 3 Time Series Regression and Exploratory Data Analysis 3.1 Exercises", " Chapter 3 Time Series Regression and Exploratory Data Analysis 3.1 Exercises # Load packages using DataFrames using Distributions using GLM using LinearAlgebra using Loess using MLLabelUtils using PyPlot using RData using StatsBase using Statistics using TimeSeries matplotlib.rcParams[&quot;text.usetex&quot;] = true; 3.1.1 Exercise 2.1 # Load data JJ = load(&quot;../data/JJ.rda&quot;); # Plot the data figure(figsize=(15, 5)) plot(JJ[&quot;jj&quot;]) xlabel(&quot;Time&quot;) ylabel(&quot;Quarterly Earnings per Share&quot;) title(&quot;Johnson &amp; Johnson quarterly earnings per share, 84 quarters, 1690-I to 1980-IV&quot;) show() Question (a) Let’s fit the regression model \\[x_t = \\beta t + \\alpha_1Q_1(t) + \\alpha_2Q_2(t) + \\alpha_3Q_3(t) + \\alpha_4Q_4(t) + w_t,\\] where \\(Q_i(t) = 1\\) if time \\(t\\) corresponds to quarter \\(i = 1, 2, 3, 4\\) and zero otherwise. # Data formatting time = -10.00:0.25:10.75 Q = repeat([1, 2, 3, 4], outer=[21]) Q_encoding = DataFrame(transpose(convertlabel(LabelEnc.OneOfK, Q))) data = hcat(DataFrame(Y=[log(x) for x in JJ[&quot;jj&quot;]], T=time), Q_encoding); # Linear model lm_model = lm(@formula(Y ~ 0 + T + x1 + x2 + x3 + x4), data); lm_model StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}} Y ~ 0 + T + x1 + x2 + x3 + x4 Coefficients: ─────────────────────────────────────────────────────────────── Coef. Std. Error t Pr(&gt;|t|) Lower 95% Upper 95% ─────────────────────────────────────────────────────────────── T 0.167172 0.00225911 74.00 &lt;1e-74 0.162676 0.171669 x1 1.05279 0.0273592 38.48 &lt;1e-52 0.998336 1.10725 x2 1.08092 0.027365 39.50 &lt;1e-53 1.02645 1.13538 x3 1.15102 0.0273825 42.03 &lt;1e-55 1.09652 1.20553 x4 0.882266 0.0274116 32.19 &lt;1e-46 0.827705 0.936828 ─────────────────────────────────────────────────────────────── Question (b) Assuming the model is correct, the estimated average quarters increase in the logged earnings per share is \\(0.17\\). Thus, the estimated average annual increase is \\(0.67\\). Question (c) Assuming the model is correct, the average logged earnings rate increase from year to year in both the third and fourth quarters. It means that for two consecutive years, the earnings rate in the third quarter of the second year will be higher than in the first year and similarly for the fourth quarter. However, for a given year, the average logged earnings rate decrease from the third to the fourth quarter. In average, during the third quarter the logged earnings rate increases by \\(15\\%\\), while it decreases by \\(18\\%\\) during the fourth quarter. (Not sure on this) Question (d) # Linear model with intercept lm_model_with_int = lm(@formula(Y ~ T + x1 + x2 + x3 + x4), data); lm_model_with_int StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}} Y ~ 1 + T + x1 + x2 + x3 + x4 Coefficients: ───────────────────────────────────────────────────────────────────────── Coef. Std. Error t Pr(&gt;|t|) Lower 95% Upper 95% ───────────────────────────────────────────────────────────────────────── (Intercept) 0.444766 1.49687e6 0.00 1.0000 -2.98004e6 2.98004e6 T 0.167172 0.00227355 73.53 &lt;1e-73 0.162646 0.171698 x1 0.608027 1.49687e6 0.00 1.0000 -2.98004e6 2.98004e6 x2 0.636149 1.49687e6 0.00 1.0000 -2.98004e6 2.98004e6 x3 0.706258 1.49687e6 0.00 1.0000 -2.98004e6 2.98004e6 x4 0.4375 1.49687e6 0.00 1.0000 -2.98004e6 2.98004e6 ───────────────────────────────────────────────────────────────────────── When an intercept is include is the model, the quarters variables are not significant anymore (but the trend coefficient is the same). The intercept hides the periodic effect express by the quarters. Without the intercept, one fit one linear model for each quarter. Question (e) # Plot the data figure(figsize=(15, 5)) plot(JJ[&quot;jj&quot;], label=&quot;Observation&quot;) plot([exp(x) for x in StatsBase.predict(lm_model)], label=&quot;Prediction&quot;) xlabel(&quot;Time&quot;) ylabel(&quot;Quarterly Earnings per Share&quot;) legend() title(&quot;Johnson &amp; Johnson quarterly earnings per share, 84 quarters, 1690-I to 1980-IV&quot;) show() # Plot the data figure(figsize=(15, 5)) scatter(1:84, data[!, &quot;Y&quot;] - StatsBase.predict(lm_model)) xlabel(&quot;Time&quot;) ylabel(&quot;Residuals&quot;) show() The model fits quite well the data. The residuals do not look exactly white, we still see some pattern in the previous graph. However, the approximation is correct. 3.1.2 Exercise 2.2 # Load data mort = load(&quot;../data/cmort.rda&quot;)[&quot;cmort&quot;]; temp = load(&quot;../data/tempr.rda&quot;)[&quot;tempr&quot;]; part = load(&quot;../data/part.rda&quot;)[&quot;part&quot;]; Question (a) # Build the model matrix time = LinRange(1970.0, 1979.7692308, 508)[5:end]; temp1 = [i - mean(temp) for i in temp][5:end]; temp2 = [(i - mean(temp))^2 for i in temp][5:end]; part1 = part[5:end]; part4 = part[1:(end - 4)]; data = DataFrame(M=mort[5:end], T=time, temp1=temp1, temp2=temp2, part1=part1, part4=part4); # Linear model with intercept lm_model = lm(@formula(M ~ T + temp1 + temp2 + part1 + part4), data); lm_model StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}} M ~ 1 + T + temp1 + temp2 + part1 + part4 Coefficients: ───────────────────────────────────────────────────────────────────────────────────── Coef. Std. Error t Pr(&gt;|t|) Lower 95% Upper 95% ───────────────────────────────────────────────────────────────────────────────────── (Intercept) 2802.96 198.462 14.12 &lt;1e-37 2413.03 3192.88 T -1.38261 0.100445 -13.76 &lt;1e-36 -1.57996 -1.18526 temp1 -0.405808 0.0352788 -11.50 &lt;1e-26 -0.475121 -0.336494 temp2 0.0215466 0.00280251 7.69 &lt;1e-13 0.0160404 0.0270528 part1 0.202882 0.0226577 8.95 &lt;1e-17 0.158366 0.247399 part4 0.103037 0.0248461 4.15 &lt;1e-4 0.0542209 0.151853 ───────────────────────────────────────────────────────────────────────────────────── The results appear to be quite good. All the coefficients are significant. # Plot the data figure(figsize=(15, 5)) plot(mort[5:end], label=&quot;Observation&quot;) plot(StatsBase.predict(lm_model), label=&quot;Prediction&quot;) xlabel(&quot;Time&quot;) ylabel(&quot;Mortality&quot;) legend() title(&quot;Cardiovascular Mortality&quot;) show() Question (b) fig = figure(figsize=(15, 5)) subplot(131) p1 = scatter(mort[5:end], temp1) xlabel(&quot;Mortality&quot;) ylabel(&quot;Temperature&quot;) subplot(132) p2 = scatter(mort[5:end], part1) xlabel(&quot;Mortality&quot;) ylabel(&quot;Particulates&quot;) subplot(133) p3 = scatter(mort[5:end], part4) xlabel(&quot;Mortality&quot;) ylabel(&quot;Particulates four weeks prior&quot;) show() # Correlation between M_t and P_t cor(mort[5:end], part1) 0.44228958011390346 # Correlation between M_t and P_{t-4} cor(mort[5:end], part4) 0.5209993034593486 \\(M_t\\) and \\(P_{t-4}\\) have a higher correlation than \\(M_t\\) and \\(P_t\\). So, using \\(P_{t-4}\\) instead of \\(P_t\\) in the linear model should results to better fit. 3.1.3 Exercise 2.3 Question (a) - Random walk fig = figure(figsize=(20, 5)) for i in 1:4 X = cumsum(rand(Normal(0.01, 1), 100)) T = 1:100 data = DataFrame(X=X, T=T) lm_model = lm(@formula(X ~ 0 + T), data); subplot(1, 4, i) plot(X, label=&quot;Observation&quot;) plot([0.1*t for t in T], label=&quot;True&quot;) plot(StatsBase.predict(lm_model), label=&quot;Prediction&quot;) xlabel(L&quot;$t$&quot;) ylabel(L&quot;$x_t$&quot;) legend() end show() Question (b) - Linear trend plus noise fig = figure(figsize=(20, 5)) for i in 1:4 T = 1:100 X = 0.1 * T + rand(Normal(), 100) data = DataFrame(X=X, T=T) lm_model = lm(@formula(X ~ 0 + T), data); subplot(1, 4, i) plot(X, label=&quot;Observation&quot;) plot([0.1*t for t in T], label=&quot;True&quot;) plot(StatsBase.predict(lm_model), label=&quot;Prediction&quot;) xlabel(L&quot;$t$&quot;) ylabel(L&quot;$x_t$&quot;) legend() end show() Question (c) The assignement shows that we should be very careful in case of correlated errors (see random walks). The estimated mean function is thus bad estimated, and without multiple realizations of the process (functional data), the mean is hardly estimated. For non-correlated errors (linear trend), all is well, and the predicted mean curve lies close to the true one. Linear models assume non-correlated errors. 3.1.4 Exercise 2.4 Let \\(y = (y_1, \\dots, y_n)\\), \\(Z \\in \\mathbb{R}^{n \\times k}\\) and \\(\\beta = \\mathbb{R}^k\\). Consider the Gaussian regression model: \\[y = Z\\beta + w, \\quad\\text{where}~ w \\sim \\mathcal{N}(0, \\sigma^2\\mathbb{1}_n).\\] Thus, we have \\(y \\sim \\mathcal{N}(Z\\beta, \\sigma^2\\mathbb{1}_n)\\) and so, \\(f(y; \\beta, \\sigma^2\\mathbb{1}_n) = (2\\pi)^{-n/2}\\text{det}(\\sigma^2\\mathbb{1}_n)^{-1/2}\\exp\\left(-\\frac{1}{2}(y - Z\\beta)^\\top \\sigma^{-2}\\mathbb{1}_n(y - Z\\beta)\\right)\\). The Kullback-Leibler Information for discriminating between two densities in the same family is given by: \\[\\begin{align} I(\\beta_1, \\sigma_1; \\beta_2, \\sigma_2) &amp;= \\frac{1}{n}\\mathbb{E}_1\\left(\\log(f(y; \\beta_1, \\sigma_1) - \\log(f(y; \\beta_2, \\sigma_2)\\right) \\\\ &amp;= \\frac{1}{2n}\\mathbb{E}_1\\left(- \\log\\text{det}(\\sigma_1^2\\mathbb{1}_n) - (y - Z\\beta_1)^\\top\\sigma_1^{-2}\\mathbb{1}_n(y - Z\\beta_1) + \\log\\text{det}(\\sigma_2^2\\mathbb{1}_n) + (y - Z\\beta_2)^\\top\\sigma_2^{-2}\\mathbb{1}_n(y - Z\\beta_2)\\right) \\\\ &amp;= \\frac{1}{2n}\\log\\left(\\frac{\\text{det}(\\sigma_2^2\\mathbb{1}_n)}{\\text{det}(\\sigma_1^2\\mathbb{1}_n)}\\right) + \\frac{1}{2n}\\mathbb{E}_1\\left((y - Z\\beta_2)^\\top\\sigma_2^{-2}\\mathbb{1}_n(y - Z\\beta_2) - (y - Z\\beta_1)^\\top\\sigma_1^{-2}\\mathbb{1}_n(y - Z\\beta_1)\\right) \\\\ &amp;= \\frac{1}{2}\\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) + \\frac{1}{2n}\\mathbb{E}_1\\left(\\text{tr}(\\sigma_2^{-2}(y - Z\\beta_2)(y - Z\\beta_2)^\\top) - \\text{tr}(\\sigma_1^{-2}(y - Z\\beta_1)(y - Z\\beta_1)^\\top)\\right) \\\\ &amp;= \\frac{1}{2}\\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) + \\frac{1}{2n}\\mathbb{E}_1\\left(\\text{tr}(\\sigma_2^{-2}(y - Z\\beta_2)(y - Z\\beta_2)^\\top)\\right) - \\frac{1}{2n}\\text{tr}(\\sigma_1^{-2}\\left(\\mathbb{E}_1(y - Z\\beta_1)(y - Z\\beta_1)^\\top)\\right) \\\\ &amp;= \\frac{1}{2}\\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) + \\frac{1}{2n}\\mathbb{E}_1\\left(\\text{tr}(\\sigma_2^{-2}(y - Z\\beta_2)(y - Z\\beta_2)^\\top)\\right) - \\frac{1}{2n}\\text{tr}(\\sigma_1^{-2}\\sigma_1^2\\mathbb{1}_n) \\\\ &amp;= \\frac{1}{2}\\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) - \\frac{1}{2} + \\frac{\\sigma_2^{-2}}{2n}\\mathbb{E}_1\\left(\\text{tr}(yy^\\top - 2y\\beta_2^\\top Z^\\top + \\beta_2^\\top Z^\\top Z\\beta_2)\\right) \\\\ &amp;= \\frac{1}{2}\\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) - \\frac{1}{2} + \\frac{\\sigma_2^{-2}}{2n}\\left(\\text{tr}(\\sigma_1^{2}\\mathbb{1}_n + \\beta_1^\\top Z^\\top Z\\beta_1 - 2Z\\beta_1\\beta_2^\\top Z^\\top + \\beta_2^\\top Z^\\top Z\\beta_2)\\right) \\\\ &amp;= \\frac{1}{2}\\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) - \\frac{1}{2} + \\frac{\\sigma_1^{2}}{2\\sigma_2^2} + \\frac{1}{2}\\frac{(\\beta_1 - \\beta_2)^\\top Z^\\top Z (\\beta_1 - \\beta_2)}{n\\sigma_2^2} \\\\ &amp;= \\frac{1}{2}\\left(\\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) - 1 + \\frac{\\sigma_1^{2}}{\\sigma_2^2} + \\frac{(\\beta_1 - \\beta_2)^\\top Z^\\top Z (\\beta_1 - \\beta_2)}{n\\sigma_2^2}\\right) \\end{align}\\] 3.1.5 Exercise 2.5 Consider to find an unbiased estimator for \\(\\mathbb{E}_1(I(\\beta_1, \\sigma_1^2; \\widehat{\\beta}, \\widehat{\\sigma}^2)).\\) \\[\\begin{align} \\mathbb{E}_1(I(\\beta_1, \\sigma_1^2; \\widehat{\\beta}, \\widehat{\\sigma}^2)) &amp;= \\frac{1}{2}\\left(\\mathbb{E}_1\\left(\\frac{\\sigma_1^2}{\\widehat{\\sigma}^2}\\right) - \\mathbb{E}_1\\log\\left(\\frac{\\sigma_1^2}{\\widehat{\\sigma}^2}\\right) - 1 + \\mathbb{E}_1\\left(\\frac{(\\beta_1 - \\widehat\\beta)^\\top Z\\top Z (\\beta_1 - \\widehat\\beta)}{n\\widehat{\\sigma}^2}\\right)\\right) \\\\ &amp;= \\frac{1}{2}\\left(n\\mathbb{E}_1\\left(\\frac{\\sigma_1^2}{n\\widehat{\\sigma}^2}\\right) - \\log(\\sigma_1^2) + \\mathbb{E}_1(\\log(\\widehat\\sigma^2)) - 1 + \\mathbb{E}_1\\left(\\frac{\\sigma_1^2}{\\sigma_1^2}\\frac{(\\beta_1 - \\widehat\\beta)^\\top Z\\top Z (\\beta_1 - \\widehat\\beta)}{n\\widehat{\\sigma}^2}\\right)\\right) \\\\ &amp;= \\frac{1}{2}\\left(\\frac{n}{n - k - 2} - \\log(\\sigma_1^2) + \\mathbb{E}_1(\\log(\\widehat\\sigma^2)) - 1 + \\mathbb{E}_1\\left(\\frac{\\sigma_1^2}{n\\widehat\\sigma^2}\\right)\\mathbb{E}_1\\left(\\frac{(\\beta_1 - \\widehat\\beta)^\\top Z\\top Z (\\beta_1 - \\widehat\\beta)}{\\sigma_1^2}\\right)\\right) \\\\ &amp;= \\frac{1}{2}\\left(- \\log(\\sigma_1^2) + \\mathbb{E}_1(\\log(\\widehat\\sigma^2)) + \\frac{n - k}{n - k - 2} - 1\\right) \\\\ \\end{align}\\] 3.1.6 Exercise 2.6 Consider a process consisting of a linear trend with an additive noise term consisting of independent random variables \\(w_t\\) with zero means and variances \\(\\sigma_w^2\\), that is \\[x_t = \\beta_0 + \\beta_1t + w_t,\\] where \\(\\beta_0, \\beta_1\\) are fixed constants. Question (a) The process \\(x_t\\) is nonstationary because its mean value function is \\(\\mu_t = \\beta_0 + \\beta_1 t\\) and thus depend on \\(t\\). Question (b) Consider the first difference series \\(\\nabla x_t = x_t - x_{t - 1}\\). Clearly, its mean value function does not depend on time \\(t\\) (equal to \\(\\beta_1\\)). The autocovariance is: \\[\\begin{align} \\gamma(s, t) &amp;= Cov(\\nabla x_s, \\nabla x_t) \\\\ &amp;= Cov(\\beta_1 + w_s - w_{s-1}, \\beta_1 + w_t - w_{t-1}) \\\\ &amp;= Cov(w_s, w_t) - Cov(w_s, w_{t-1}) - Cov(w_{s-1}, w_t) + Cov(w_{s-1}, w_{t-1}) \\\\ &amp;= \\left\\{ \\begin{array}{r c l} 2\\sigma_w^2 &amp;if&amp; s - t = 0\\\\ -\\sigma_w^2 &amp;if&amp; \\vert s - t \\vert = 1\\\\ \\end{array} \\right., \\end{align}\\] and thus depends on \\(s\\) and \\(t\\) only through their difference \\(\\lvert s - t \\rvert\\). And, so the process is stationary. Question (c) We replace \\(w_t\\) by a general stationary process \\(y_t\\), with mean \\(\\mu_y\\) and autocovariance function \\(\\gamma_y(h)\\). Clearly, the mean value of the process \\(\\nabla x_t\\) is still \\(\\beta_1\\) and does not depend on time \\(t\\). The autocovariance is \\(\\gamma(h) = 2\\gamma_y(h) - \\gamma_y(h + 1) - \\gamma_y(h - 1)\\). It depends only on \\(h\\), so this process is also stationary. 3.1.7 Exercise 2.7 We consider the process \\(\\nabla x_t = \\delta + w_t + y_t - y_{t-1}\\) where \\(w_t \\sim \\mathcal{N}(0, \\sigma^2)\\). Noting that \\((w_t)_t\\) and \\((y_t - y_{t-1})_t\\) are independant, and using the previous exercise, \\(\\nabla x_t\\) is a stationary process. 3.1.8 Exercise 2.8 # Load data varve = load(&quot;../data/varve.rda&quot;)[&quot;varve&quot;]; log_varve = [log(x) for x in varve]; # Plot the data figure(figsize=(15, 5)) plot(varve) xlabel(&quot;Years&quot;) ylabel(&quot;Thicknesses&quot;) title(&quot;Glacial varve thicknesses from Massachusetts&quot;) show() # Plot the log data figure(figsize=(15, 5)) plot(log_varve) xlabel(&quot;Years&quot;) ylabel(&quot;log-Thicknesses&quot;) title(&quot;Glacial varve log-thicknesses from Massachusetts&quot;) show() Question (a) fig = figure(figsize=(15, 5)) subplot(121) p1 = hist(varve, density=true) xlabel(&quot;Thickness&quot;) ylabel(&quot;Density&quot;) title(&quot;Variance: 1st half - $(var(varve[1:317])) 2nd half - $(var(varve[318:634]))&quot;) subplot(122) p2 = hist(log_varve, density=true) xlabel(&quot;log-Thickness&quot;) ylabel(&quot;Density&quot;) title(&quot;Variance: 1st half - $(var(log_varve[1:317])) 2nd half - $(var(log_varve[318:634]))&quot;) show() We see that the serie exhibits heteroscedasticity thanks to the computation of the variance. The log-transformation stabilized a bit the variance within the data. Morover, the normality assumption is more reflected in the log-data. Question (b) Regarding the plot of the series \\(y_t\\), it seems that the series decrease for the first \\(200\\) years, then increase for \\(250\\) years, and finally decrease until the end. Question (c) # Plot the ACF figure(figsize=(15, 5)) stem(0:633, autocor(log_varve, 0:633), use_line_collection=true) hlines([1.96 / sqrt(634), -1.96 / sqrt(634)], xmin=0, xmax=634, colors=&quot;red&quot;, linestyles=&quot;dashed&quot;) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() The autocorrelation is still quite important until \\(h = 500\\). Note that between \\(h = 100\\) and \\(h = 350\\), \\(y_t\\) and \\(y_{t + h}\\) are negatively correlated. The autocorrelation does not really belong to the \\(95\\%\\) CI (red dashd lines), and there is clearly not an exponential decay of the ACF. So, the series is not stationary. Question (d) u = log_varve[2:end] - log_varve[1:end - 1]; # Plot the log data figure(figsize=(15, 5)) plot(u) xlabel(&quot;Years&quot;) title(&quot;Glacial varve log-thicknesses first difference from Massachusetts&quot;) show() # Plot the ACF figure(figsize=(15, 5)) stem(0:632, autocor(u, 0:632), use_line_collection=true) hlines([1.96 / sqrt(634), -1.96 / sqrt(634)], xmin=0, xmax=634, colors=&quot;red&quot;, linestyles=&quot;dashed&quot;) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function&quot;) show() The autocovariance function decrease rapidly the \\(95\\%\\) CI (red dashed lines). This series is a reasonably stationary series. It is an estimation of \\(x_t - x_{t-1} / x_{t-1}\\) when this quantity is small. This can be usefull for the analysis of the return. Question (e) The series \\(u_t\\) is assumed to be stationary. So, its mean function does not depend on time. Moreover, it has been build by differentiate two series. So, its seems reasonable to assume the following expansion for \\(u_t\\): \\[u_t = \\mu + w_t + \\theta w_{t-1}.\\] This is a direct generalisation of the example \\(1.26\\) with \\(w_t\\) a continuous random variable. We assume that the \\(w_t\\) are independent with mean \\(0\\) and variance \\(\\sigma_w^2\\). \\[\\begin{align} \\gamma(s, t) &amp;= Cov(u_s, u_t) \\\\ &amp;= Cov(\\mu + w_s + \\theta w_{s-1}, \\mu + w_t + \\theta w_{t-1}) \\\\ &amp;= Cov(w_s, w_t) +\\theta Cov(w_s, w_{t-1})+\\theta Cov(w_{s-1}, w_t)+\\theta^2 Cov(w_{s-1}, w_{t-1}) \\\\ &amp;= \\left\\{ \\begin{array}{r c l} (1 + \\theta^2)\\sigma_w^2 &amp;if&amp; s - t = 0\\\\ \\theta \\sigma_w^2 &amp;if&amp; \\vert s - t \\vert = 1\\\\ 0 &amp;if&amp; \\vert s - t \\vert &gt; 1 \\end{array} \\right. \\end{align}\\] Question (f) Considering \\(\\rho_u(1)\\) and \\(\\gamma_u(0)\\), we found out that \\[\\theta^2 \\rho_u(1) - \\theta + \\rho_u(1) = 0.\\] Thus, resolving the second order polynmials, we have \\[\\theta = \\frac{1 \\pm \\sqrt{1 - 4\\rho_u(1)}}{2\\rho_u(1)}.\\] Finally, by replacing this expression within \\(\\gamma_u(0)\\), we have an expression for \\(\\sigma_w^2\\) \\[\\sigma_w^2 = \\left(1 + \\frac{1 \\pm \\sqrt{1 - 4\\rho_u(1)}}{2\\rho_u^2(1)} - \\frac{1}{\\rho_u(1)}\\right)^{-1}\\gamma_u(0).\\] 3.1.9 Exercise 2.9 # Load data SOI = load(&quot;../data/soi.rda&quot;)[&quot;soi&quot;]; Question (a) # Build the model matrix time = LinRange(1, 453, 453); data = DataFrame(S=SOI, T=time); # Linear model lm_model = lm(@formula(S ~ 0 + T), data); # Plot the data figure(figsize=(15, 5)) plot(SOI, label=&quot;Observation&quot;) plot(StatsBase.predict(lm_model), label=&quot;Trend&quot;) xlabel(&quot;Time&quot;) ylabel(&quot;SOI&quot;) legend() title(&quot;Southern Oscillation Index&quot;) show() It does not appear a significant trend in the sea surface temperature in the data. SOI_detrended = SOI - StatsBase.predict(lm_model); Question (b) p = periodogram(SOI_detrended, fs=1); figure(figsize=(15, 5)) plot(p.freq, p.power) xlabel(&quot;Frenquency&quot;) ylabel(&quot;Periodogram&quot;) show() big_perio = sortperm(p.power, rev=true); # Three most important frequency [1 / x for x in p.freq[big_perio][1:3]] 3-element Array{Float64,1}: 12.0 240.0 60.0 So, the most important peak correspond to one cycle every \\(12\\), \\(240\\) and \\(60\\) months. The cycle every \\(60\\) months (five years) reflect the El Nino effect. The one that appears every \\(240\\) months (\\(20\\) years) may reflect a particularly important and intense El Nino effect. 3.1.10 Exercise 2.10 # Load data gas = load(&quot;../data/gas.rda&quot;)[&quot;gas&quot;]; oil = load(&quot;../data/oil.rda&quot;)[&quot;oil&quot;]; Question (a) # Plot the data figure(figsize=(15, 5)) plot(gas, label=&quot;Gas&quot;) plot(oil, label=&quot;Oil&quot;) xlabel(&quot;Week&quot;) legend() title(&quot;Gas and Oil series&quot;) show() These series resemble the most to random walks with linear trends. The series are probably not stationary, because their mean functions depend on time. Question (b) The transformation \\(y_t = \\nabla \\log x_t\\) is an approximation of the return \\(\\nabla x_t / x_t\\) when \\(x_t\\) is small. The computation with the log is probably more stable. Question (c) gas_log = [log(x) for x in gas]; oil_log = [log(x) for x in oil]; gas_r = gas_log[2:end] - gas_log[1:end - 1]; oil_r = oil_log[2:end] - oil_log[1:end - 1]; # Plot the data figure(figsize=(15, 5)) plot(gas_r, label=&quot;Gas&quot;) plot(oil_r, label=&quot;Oil&quot;) xlabel(&quot;Week&quot;) legend() title(&quot;Returned Gas and Oil series&quot;) show() # Plot the ACF for gas return figure(figsize=(15, 5)) stem(0:543, autocor(gas_r, 0:543), use_line_collection=true) hlines([1.96 / sqrt(544), -1.96 / sqrt(544)], xmin=0, xmax=544, colors=&quot;red&quot;, linestyles=&quot;dashed&quot;) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function for Gas&quot;) show() # Plot the ACF for oil return figure(figsize=(15, 5)) stem(0:543, autocor(oil_r, 0:543), use_line_collection=true) hlines([1.96 / sqrt(544), -1.96 / sqrt(544)], xmin=0, xmax=544, colors=&quot;red&quot;, linestyles=&quot;dashed&quot;) xlabel(L&quot;$h$&quot;) title(&quot;Autocorrelation function for Oil&quot;) show() The transformation of the data make the series stationary. The correlation between the Gas and Oil series is likely to be high. Question (d) # Plot the CCF for oil return figure(figsize=(15, 5)) stem(0:543, crosscor(gas_r, oil_r, 0:543), use_line_collection=true) hlines([1.96 / sqrt(544), -1.96 / sqrt(544)], xmin=0, xmax=544, colors=&quot;red&quot;, linestyles=&quot;dashed&quot;) xlabel(L&quot;$h$&quot;) title(&quot;Cross-correlation function between Oil and Gas series&quot;) show() The correlation when \\(h = 0\\) is high (around \\(0.7\\)) and indicate the influence between the two series. Moreover, the small correlation values when gas leads oil is significant. Question (e) x_pred = range(extrema(gas_r)...; step = 0.1) fig = figure(figsize=(20, 5)) subplot(141) p1 = scatter(gas_r[1:end], oil_r[1:end]) model = loess(gas_r[1:end], oil_r[1:end]) plot(x_pred, Loess.predict(model, x_pred), c=&quot;r&quot;) xlabel(&quot;Gas growth rate&quot;) ylabel(&quot;Oil growth rate&quot;) subplot(142) p2 = scatter(gas_r[2:end], oil_r[1:end - 1]) model = loess(gas_r[2:end], oil_r[1:end - 1]) plot(x_pred, Loess.predict(model, x_pred), c=&quot;r&quot;) xlabel(&quot;Gas growth rate&quot;) ylabel(&quot;Oil growth rate one week prior&quot;) subplot(143) p3 = scatter(gas_r[3:end], oil_r[1:end - 2]) model = loess(gas_r[3:end], oil_r[1:end - 2]) plot(x_pred, Loess.predict(model, x_pred), c=&quot;r&quot;) xlabel(&quot;Gas growth rate&quot;) ylabel(&quot;Oil growth rate two week prior&quot;) subplot(144) p4 = scatter(gas_r[4:end], oil_r[1:end - 3]) model = loess(gas_r[4:end], oil_r[1:end - 3]) plot(x_pred, Loess.predict(model, x_pred), c=&quot;r&quot;) xlabel(&quot;Gas growth rate&quot;) ylabel(&quot;Oil growth rate three week prior&quot;) show() We have fit the data using the lowess method, related to nearest neighbor regression. We can see few outliers in the data. Only the first model exhibit almost linear relationship. Question (f) Consider the model \\[G_t = \\alpha_1 + \\alpha_2 I_t + \\beta_1 O_t + \\beta_2 O_{t-1} + w_t,\\] where \\(I_t = 1\\) if \\(O_t \\geq 0\\) and \\(0\\) otherwise. \\(I_t\\) is the indicator of no growth or positive growth in oil price. I = [x &gt; 0 for x in oil_r] data = DataFrame(G=gas_r[2:end], I=I[2:end], O=oil_r[2:end], O_lag=oil_r[1:end-1]); # Linear model lm_model = lm(@formula(G ~ I + O + O_lag), data); lm_model StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}} G ~ 1 + I + O + O_lag Coefficients: ──────────────────────────────────────────────────────────────────────────────── Coef. Std. Error t Pr(&gt;|t|) Lower 95% Upper 95% ──────────────────────────────────────────────────────────────────────────────── (Intercept) -0.00610993 0.00345539 -1.77 0.0776 -0.0128976 0.000677756 I 0.0117854 0.00551434 2.14 0.0330 0.000953159 0.0226176 O 0.687749 0.0583795 11.78 &lt;1e-27 0.57307 0.802429 O_lag 0.112152 0.0385698 2.91 0.0038 0.0363868 0.187918 ──────────────────────────────────────────────────────────────────────────────── The fitted model when there is negative growth in oil price at time \\(t\\) is \\[G_t = -0.006 + 0.68 O_t + 0.11 O_{t-1}.\\] The fitted model when there is no or positive growth in oil price at time \\(t\\) is \\[G_t = 0.0057 + 0.68 O_t + 0.11 O_{t-1}.\\] Thus, the model is (almost) symetric considering the indicator of growth in oil price. The asymmetry assumption does not seem to hold here. figure(figsize=(15, 5)) plot(gas_r[2:end], label=&quot;Gas&quot;) plot(StatsBase.predict(lm_model), label=&quot;Prediction&quot;) xlabel(&quot;Week&quot;) legend() title(&quot;Returned Gas and its prediction&quot;) show() resid = gas_r[2:end] - StatsBase.predict(lm_model) fig = figure(figsize=(15, 5)) scatter(1:543, resid) xlabel(L&quot;$t$&quot;) ylabel(&quot;Residuals&quot;) show() The residuals appear to be normal, and look the original series. Don’t know how to analyze that. 3.1.11 Exercise 2.11 # Load data globtemp = load(&quot;../data/globtemp.rda&quot;)[&quot;globtemp&quot;]; function filter_convolution(x, filter, sides) nx = length(x) nf = length(filter) nshift = (sides == 1) ? 0 : div(nf, 2) out = Array{Float64}(undef, nx) for i in 1:nx z = 0 if (i + nshift - nf &lt; 0) || (i + nshift &gt; nx) out[i] = NaN continue end for j in (max(1, i + nshift - nx)):(min(nf, i + nshift - 1)) tmp = x[i + nshift - j] z += filter[j] * tmp end out[i] = z end return out end filter_convolution (generic function with 1 method) x_pred = range(extrema(1:136)...; step = 0.1) model = loess(1:136, globtemp) fig = figure(figsize=(15, 5)) p1 = scatter(1:136, globtemp, label = &quot;Obs&quot;) plot(x_pred, Loess.predict(model, x_pred), c=&quot;r&quot;, label = &quot;Loess&quot;) plot(1:136, filter_convolution(globtemp, [0.25, 0.25, 0.25, 0.25], 2), c=&quot;g&quot;, label=&quot;Moving Average&quot;) xlabel(&quot;Years&quot;) ylabel(&quot;Temperature deviations&quot;) legend() title(&quot;Global mean land-ocean temperature deviations to 2015&quot;) show() We see an increasing linear trend in the data. The two smoothing methods exhibit different results. The loess method results in a smoother estimation of the trend. "],["methods.html", "Chapter 4 Methods", " Chapter 4 Methods We describe our methods in this chapter. "],["applications.html", "Chapter 5 Applications 5.1 Example one 5.2 Example two", " Chapter 5 Applications Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References Shumway, Robert, and David Stoffer. 2011. Time Series Analysis and Its Applications with r Examples. Time Series Analysis and Its Applications: With R Examples. Vol. 9. https://doi.org/10.1007/978-1-4419-7865-3. "]]
